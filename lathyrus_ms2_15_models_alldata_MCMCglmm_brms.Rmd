---
title: "Lathyrus ms2: selection on reaction norms for flowering time"
subtitle: "Models with all data performed with MCMCglmm and brms"
author : "Alicia Vald√©s"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_notebook:
    toc: yes
    toc_depth: '4'
    latex_engine: xelatex
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r load packages, include=FALSE}
library(tidyverse)
library(tidyr)
library(MCMCglmm)
library(brms)
library(parallel)
library(beepr)
library(ggthemes)
library(knitr)
library(bayesplot)
library(tidybayes)
library(future)
```

```{r Define ggplot themes, include=FALSE}
my_theme <- function(){
  theme_base()+theme(plot.background=element_rect(fill="white", colour=NA))+
  theme(legend.position="none")+theme(text=element_text(family="serif"))+
  theme(plot.title = element_text(hjust =-0.06))
}
my_theme_legend <- function(){
  theme_base()+theme(plot.background=element_rect(fill="white", colour=NA))+
  theme(text=element_text(family="serif"))+
  theme(plot.title = element_text(hjust =-0.06))
}
```

```{r load models, include=FALSE}
# Load previously run models and stuff
load("output/large_objects.RData")
```

# Read data and check ns

```{r}
datadef<-read.csv("data/datadef.csv") 
head(datadef)
```

Number of individuals in each period:

```{r}
length(with(subset(datadef,period=="old"),unique(id)))
length(with(subset(datadef,period=="new"),unique(id)))
```

Number of observations in each period:

```{r}
nrow(subset(datadef,period=="old"))
nrow(subset(datadef,period=="new"))
```

Number of cases with FFD in each period:

```{r}
nrow(subset(datadef,period=="old"&!is.na(FFD)))
nrow(subset(datadef,period=="new"&!is.na(FFD)))
```

# Data preparation

```{r data prep MCMCglmm, message=FALSE, warning=FALSE}
datadef_total<-datadef %>%
  group_by(id)%>%
  # Calculate mean fitness per year of study 
  # and mean fitness per flowering event
  summarise(mean_fitness_study=sum(intactseed,na.rm=T)/mean(n_years_study),
            mean_fitness_fl=sum(intactseed,na.rm=T)/mean(n_years_fl_fitness))%>%
   arrange(.,id) # Order by id

with(datadef_total,cor(mean_fitness_study,mean_fitness_fl))  # Highly corr (0.87)

# Calculate mean shoot volume for each id using values of shoot volume 
# for all ids/years (including flowering and non-flowering years)

shoot_vol_all_means<-datadef[c(1,3,10)]%>%
  group_by(id)%>%
  summarise(shoot_vol_mean=mean(shoot_vol,na.rm=T)) 
# Mean of all available values 

# Join shoot volume data
datadef_total<-datadef_total%>%left_join(shoot_vol_all_means)%>%
  left_join(unique(datadef[c(2,3,11)]))
head(datadef_total)
nrow(subset(datadef_total,is.na(shoot_vol_mean))) 
# 46 ids with no info on shoot volume

# Add first_yr to total data + 
# Year column is only relevant for FFD, but is set to first_yr for fitness values
datadef_total$first_yr<-ifelse(grepl("old",as.character(datadef_total$id)),
                               1987,2006)

# Using sqrt of mean shoot volume over all years when available, centered
datadef_total<-datadef_total%>%
  mutate(shoot_vol_mean_sqrt=sqrt(shoot_vol_mean),
         cn_shoot_vol_mean_sqrt=scale(shoot_vol_mean_sqrt,center=T,scale=F))
```

Compare distributions of mean fitness per year of study and mean fitness per flowering event between old and new periods:

```{r message=FALSE, warning=FALSE}
ggplot(datadef_total,aes(x=mean_fitness_study))+
  geom_histogram(colour="black",fill="white",position="dodge")+
  facet_wrap(~period,scales="free_y")+
  geom_vline(data=plyr::ddply(datadef_total,"period",summarise,
                        mean_fitness_study.mean=mean(mean_fitness_study)),
             aes(xintercept=mean_fitness_study.mean),
             linetype="dashed", size=1, colour="red")
ggplot(datadef_total,aes(x=mean_fitness_fl))+
  geom_histogram(colour="black",fill="white",position="dodge")+
  facet_wrap(~period,scales="free_y")+
  geom_vline(data=plyr::ddply(datadef_total,"period",summarise,
                        mean_fitness_fl.mean=mean(mean_fitness_fl)),
             aes(xintercept=mean_fitness_fl.mean),
             linetype="dashed", size=1, colour="red")
```

Distributions and means of the two mean fitness measures are similar among the two periods.

# A) MCMCglmm models

## Univariate models

Code based on Arnold et al. 2019 Phil. Trans. R. Soc. B.

```{r sc and priors}
# Scaling factor for MCMCglmm iterations
sc <- 1000 # Increase this parameter for longer runs

priorUV2 <- list(G = list(G1 = list(V = diag(1), nu = 1),
                          # for random effect of year
                          G2 = list(V = diag(1), nu = 1)),
                 # for random effect of id 
                 R = list(R1 = list(V = diag(1), nu = 2))) 
priorUV2_RR <- list(G = list(G1 = list(V = diag(1), nu = 1),  
                             # other random effect (YEAR)
                             G2 = list(V = diag(2), nu = 1)),
                    # ^ 2x2 var-covar matrix for var in slopes + intercepts
                    R = list(R1 = list(V = diag(1), nu = 2)))  
```

### FFD with random effects of year and individual-intercept

```{r MCMCglmm univar 1, eval=FALSE, include=TRUE}
univar.FFD.all <- MCMCglmm(FFD ~ cmean_4,
                     random = ~year + id,
                     rcov = ~units,
                     data = datadef,prior = priorUV2, 
                     family = "gaussian",nitt = 2100 * sc, 
                     thin = sc, burnin = 100 * sc, verbose = F)
# nitt = burnin + thin*(n samples to keep)
# Aim to store 2000 iterations
```

```{r MCMCglmm univar 2}
summary(univar.FFD.all)
```

### Random regression for FFD, including random effects of individual slopes and covariance between intercept and slope

```{r MCMCglmm univar 3, eval=FALSE, include=TRUE}
univar.FFD_RR.all <- MCMCglmm(FFD ~ cmean_4,
                        random = ~year + us(1 + cmean_4):id,
                        rcov = ~units,
                        data = datadef,prior = priorUV2_RR, 
                        family = "gaussian",nitt = 2100*sc,
                        thin = sc, burnin = 100*sc, verbose = F,pr=T) 
# pr= T saves the posterior distribution of the individual random effects
# (analagous to the BLUP from the LMM)
```

```{r MCMCglmm univar 4}
summary(univar.FFD_RR.all)
```

#### Extract BLUPs from this model

Code adapted from Houslay & Wilson 2017 Behav. Ecol. Code for graphs based on Arnold et al. 2019 Phil. Trans. R. Soc. B.

```{r extract BLUPs, fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
BLUPs_MCMC.all <- tibble(Trait = attr(colMeans(univar.FFD_RR.all$Sol), "names"),
                          Value = colMeans(univar.FFD_RR.all$Sol)) %>%
  filter(grepl("id", Trait))%>% # Select only id intercepts and slopes
  mutate(type=ifelse(grepl("Intercept",Trait),"intercept","slope"))%>%
  mutate(id=sub(".*id.", "", Trait))%>%
  select(-Trait)%>%
  spread(., type, Value) # Convert from long to wide
with(BLUPs_MCMC.all,cor(intercept,slope)) # highly correlated!
```

Correlation among intercepts and slopes

```{r corr1, fig.height=2.5, fig.width=6}
univar.FFD_RR.all_intslope <- 
  univar.FFD_RR.all$VCV[,"cmean_4:(Intercept).id"]/
(sqrt(univar.FFD_RR.all$VCV[,"(Intercept):(Intercept).id"])*
sqrt(univar.FFD_RR.all$VCV[,"cmean_4:cmean_4.id"]))
posterior.mode(univar.FFD_RR.all_intslope)
HPDinterval(univar.FFD_RR.all_intslope) 
```

Plots with BLUPs

```{r, echo=FALSE, fig.height=4, fig.width=10}
ggplot(BLUPs_MCMC.all, aes(id, intercept)) + 
  geom_point(aes(group = id, colour = id), size = 4, alpha = 0.5) + 
  ylab("BLUP intercept estimate") +
  geom_hline(yintercept = 0, lty = 2) + my_theme() +
  theme(axis.text.x = element_blank())
```

```{r, echo=FALSE, fig.height=4, fig.width=10}
ggplot(BLUPs_MCMC.all, aes(id, slope)) + 
  geom_point(aes(group = id, colour = id), size = 4, alpha = 0.5) + 
  ylab("Plasticity (BLUP slope estimate)") +
  geom_hline(yintercept = 0, lty = 2) + my_theme() +
  theme(axis.text.x = element_blank())
```

```{r BLUPs_all20, echo=FALSE, fig.height=4, fig.width=4}
ggplot(BLUPs_MCMC.all, aes(intercept, slope)) +
  geom_point(aes(group = id, colour = id), size = 2, alpha = 0.5) +
  xlab("BLUP intercept estimate") +
  ylab("BLUP slope estimate") + my_theme()
```

```{r BLUPs_all21, echo=FALSE, fig.height=4, fig.width=10}
BLUPs_MCMC.all$id_ordered <- factor(BLUPs_MCMC.all$id,
                                    levels = BLUPs_MCMC.all$id[order(BLUPs_MCMC.all$slope)])
ggplot(BLUPs_MCMC.all, aes(id_ordered, slope)) +
  geom_bar(stat = "identity", aes(group = id, fill = id)) +
  xlab("Id (in ranked order of plasticity)") +
  ylab("Plasticity (BLUP slope estimate)") +
  my_theme() + theme(axis.text.x = element_blank())
```

## Bivariate models

Code based on Arnold et al. 2019 Phil. Trans. R. Soc. B.

Fitting bivariate models of fitness and FFD, with random regressions for individuals, using a Poisson distribution for fitness (and using absolute fitness). Using mean April temperature. Using either mean fitness per year of study (dividing sum of fitness by the number of years that each plant was included in the study) or mean fitness per flowering event (dividing sum of fitness by the number of years that each plant flowered and which had fitness information available). Including / not including mean shoot volume over all years with available data (with an effect on fitness) as a condition variable.

### 1. mean_fitness_fl, no condition variable

Stack data:

```{r}
# Create a single data-set "datadef.stack1", with single column at start 
# to index observations
datadef.stack1 <- c()
datadef.stack1$Obs <- 1:(837 + 2478) 
datadef.stack1$id <- c(as.character(datadef_total$id),
                      as.character(subset(datadef,!is.na(FFD))$id)) 
# ids in alphabetical order

# Year column is only relevant for FFD, but is set to first_yr for fitness values
datadef.stack1$year <- c(datadef_total$first_yr,
                         subset(datadef,!is.na(FFD))$year)

# Temperature column is only relevant for FFD, but is set to 0 for fitness values
datadef.stack1$temp <- c(rep(0, 837), subset(datadef,!is.na(FFD))$cmean_4)

# Create single column with first fitness values (ABSOLUTE VALUES), then FFD values:
datadef.stack1$fitness.FFD.stack <- c(round(datadef_total$mean_fitness_fl),
                                     subset(datadef,!is.na(FFD))$FFD)

# Create 3 index columns needed for MCMCglmm
datadef.stack1$traits <- as.factor(c(rep("fitness", 837), rep("FFD", 2478)))
datadef.stack1$variable <- as.factor(datadef.stack1$traits)
# Fitness will be modelled with an overdispersed Poisson 
# FFD will be modelled with a Gaussian distribution
# Specify this with the column 'family':
datadef.stack1$family <- c(rep("poisson", 837), rep("gaussian", 2478))
datadef.stack1 <- data.frame(datadef.stack1)

datadef.stack1$id <- as.factor(datadef.stack1$id)
datadef.stack1$year <- as.factor(datadef.stack1$year)
head(datadef.stack1)
```

```{r}
# Scaling factor for MCMCglmm iterations
sc <- 1000 # Increase this parameter for longer runs

priorBiv <- list(G = list(G1 = list(V = diag(1), nu = 1)),
                    # ^ random effect for year (fitted for FFD only)
                    R = list(R1 = list(V = diag(3), nu = 3, covu = TRUE), 
                             # ^ 3-way var-cov matrix of (id + temp:id) for FFD,
                             # residual for fitness
                             R2 = list(V = diag(1), nu = 1))) # residual for FFD
```

```{r MCMCglmm models 1, eval=FALSE, include=TRUE}
bivar1.all <- MCMCglmm(fitness.FFD.stack ~ variable - 1 +
                         # ^ means for each variable 
                         # (and no overall mean (hence "-1"))
                         at.level(variable, "FFD"):temp,  
                       # single fixed effect of temp
                       random = ~us(at.level(variable, "FFD")):year +
                         us(at.level(variable, "FFD") + 
                              at.level(variable,"FFD"):temp):id,
                       # ^ random intercepts for individual, 
                       # and random slopes for temp|id
                       rcov = ~us(at.level(variable, "fitness")):id + 
                         # ^ variance between indivdiuals in fitness
                         # (which is residual variance)
                         us(at.level(variable, "FFD")):Obs,    
                         # ^ residual variance within indivdiuals between years 
                       # (labelled by 'Obs')
                       data = datadef.stack1,
                       prior = priorBiv, 
                       family = NULL, # specified already in the data-set
                       nitt = 2100 * sc, thin = sc, burnin = 100 * sc, 
                       verbose = F,singular.ok = T) 
# nitt = burnin + thin*(n samples to keep)
# Aim to store 2000 iterations
```

```{r}
kable(summary(bivar1.all)$solutions,digits=c(3,3,3,0,3),caption="Fixed effects")
kable(summary(bivar1.all)$Gcovariances,digits=c(3,3,3,0),caption="Random effects")
kable(summary(bivar1.all)$Rcovariances,digits=c(3,3,3,0),caption="Random effects")
kable(diag(autocorr(bivar1.all$Sol)[2, , ]),caption="Autocorrelation")
kable(diag(autocorr(bivar1.all$VCV)[2, , ]),caption="Autocorrelation")
```

For interpretation of covariances, we convert them to correlations using the formula for a correlation with the posterior distributions of our (co)variance components. This gives us a distribution of correlation values that we can use to calculate estimates and 95% credible intervals (code adapted from Houslay & Wilson 2017 Behav. Ecol.).

Among-individual correlation between intercepts and slopes for FFD, between FFD and fitness and between fitness and variation in slopes for FFD:

```{r MCMCglmm models 6, fig.height=2.5, fig.width=6}
cor_bivar1.all_intslope <- 
  bivar1.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\").id"]/
(sqrt(bivar1.all$VCV[,"at.level(variable, \"FFD\").id:at.level(variable, \"FFD\").id"])*
sqrt(bivar1.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\"):temp.id"]))
posterior.mode(cor_bivar1.all_intslope)
HPDinterval(cor_bivar1.all_intslope) 

cor_bivar1.all_intfit <-
  bivar1.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"FFD\").id"]/
  (sqrt(bivar1.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"fitness\").id"])*
     sqrt(bivar1.all$VCV[,"at.level(variable, \"FFD\").id:at.level(variable, \"FFD\").id"]))
posterior.mode(cor_bivar1.all_intfit)
HPDinterval(cor_bivar1.all_intfit)

cor_bivar1.all_slopefit <- 
  bivar1.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"FFD\"):temp.id"]/
  (sqrt(bivar1.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"fitness\").id"])*
     sqrt(bivar1.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\"):temp.id"]))
posterior.mode(cor_bivar1.all_slopefit)
HPDinterval(cor_bivar1.all_slopefit)
```

Correlation between intercepts and slopes for FFD is smaller than in univariate models. Why?

Intercepts and slopes of RNs are positively correlated: Plants that flower earlier on average are also more responsive to temperature. Fitness is negatively correlated with the intercept, but not with the slope of the RN: individuals that flower earlier on average have higher fitness, but responsiveness to temperature does not seem to affect fitness.

##### Extract selection coefficients

Selection differentials or gradients should be calculated using relative fitness, and models are typically fitted assuming Gaussian errors. However, where the fitness measure follows a non-Gaussian distribution, as is typically the case with skewed distributions of fitness, a GLMM of absolute fitness will be preferable. The resulting covariances returned by the model will then be between the trait on the data scale and fitness on a 'latent' (link-function) scale. These estimates need to be transformed if data-scale estimates of selection are required. However, in the case of a GLMM with a log-link function (e.g. Poisson here), it is possible to exploit the fact that the latent-scale covariance with absolute fitness is equivalent to the data-scale covariance of relative fitness: consequently, and conveniently, the covariance components of the var-covar matrix on the latent scale can simply be treated as selection differentials S. By extension, estimates of selection gradients will also provide data-scale selection gradients.

```{r fig.height=2.5, fig.width=6}
# Extract 3x3 matrix of variance-covariance values for intercepts and slopes 
# of temp, and fitness 
# These are in the 2nd-10th columns of model output
P.bivar1.all <- bivar1.all$VCV[,2:10]         
P.bivar1.all.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.bivar1.all.mode[k] <- posterior.mode(P.bivar1.all[,k])
P.bivar1.all.mode

# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.bivar1.all <- bivar1.all$VCV[, c(4,7)]
S.bivar1.all <- P.bivar1.all[, c(3,6)] # This is exactly the same as above
colnames(S.bivar1.all) <- c("S_intercepts", "S_slopes")
S.bivar1.all.mode <- P.bivar1.all.mode[1:2, 3]
S.bivar1.all.mode
posterior.mode(mcmc(S.bivar1.all)) # This is exactly the same as above
HPDinterval(mcmc(S.bivar1.all))

# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
n <- length(bivar1.all$VCV[,2])   # sample size
beta_post_bivar1.all <- matrix(NA, n ,2)

for (i in 1:n) {
  P3 <- matrix(rep(NA, 9), nrow = 3)  
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3[k] <- P.bivar1.all[i, k] }  
  P2 <- P3[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S <- P3[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_bivar1.all[i,] <- solve(P2) %*% S   # selection gradients beta = P^-1 * S
}

# Finally, extract and plot the selection gradients posterior modes 
# and 95% credible intervals for both selection on intercepts (trait value) 
# and slopes (trait plasticity).

colnames(beta_post_bivar1.all) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_bivar1.all))
HPDinterval(mcmc(beta_post_bivar1.all))
```

The selection differentials are "significant" for RN intercept (negative), but not for RN slope. The selection gradients are significant for both RN intercept (negative) and slope (positive). This means that there is significant total and direct selection on the intercept of the RN, selecting for an earlier flowering time on average. Not sure how to interpret the selection on the slope though. The selection differential is not significant, meaning that there is no total selection on the slope, but the selection gradient is significant and positive. I guess this means that, after correcting for the covariance between intercepts and slopes, there is significant selection on the slope. And the selection gradient for the slope being positive means that there is selection for more positive slopes (i.e. less negative = individuals less responsive to temperature, because the relationship among FFD and temperature is negative: earlier flowering (lower FFD) with higher temperatures). Am I interpreting this correctly?

### 2. mean_fitness_fl, with shoot volume

Stack data:

```{r}
# Create a single data-set "datadef.stack2", with single column at start 
# to index observations
datadef.stack2 <- c()
nrow(subset(datadef_total,!is.na(shoot_vol_mean))) 
# 791 ids with info on shoot_vol

nrow(subset(datadef,!is.na(FFD)&
              id %in% subset(datadef_total,!is.na(shoot_vol_mean))$id)) 
# 2432 cases with info on FFD and fitness 
# corresponding to the 791 ids with info on shoot_vol

# Check that those cases correspond to those 791 individuals
length(unique(subset(datadef,!is.na(FFD)&
         id %in% subset(datadef_total,!is.na(shoot_vol_mean))$id)$id))

datadef.stack2$Obs <- 1:(791 + 2432) 
datadef.stack2$id <- c(as.character(subset(datadef_total,
                                           !is.na(shoot_vol_mean))$id),
                      as.character(subset(datadef,!is.na(FFD)&
                            id %in% subset(datadef_total,!is.na(shoot_vol_mean))$id)$id)) 
# ids in alphabetical order

# Year column is only relevant for FFD, but is set to first_yr for fitness values
datadef.stack2$year <- c(subset(datadef_total,!is.na(shoot_vol_mean))$first_yr,
                         subset(datadef,!is.na(FFD)&
                                  id %in%subset(datadef_total,
                                                !is.na(shoot_vol_mean))$id)$year)

# Temperature column is only relevant for FFD, but is set to 0 for fitness values
datadef.stack2$temp <- c(rep(0, 791), 
                         subset(datadef,!is.na(FFD)&
                                  id %in%subset(datadef_total,
                                                !is.na(shoot_vol_mean))$id)$cmean_4)

# Shoot volume column is only relevant for fitness, but is set to 0 for FFD values
datadef.stack2$cn_shoot_vol <- c(subset(datadef_total,
                                        !is.na(shoot_vol_mean))$cn_shoot_vol_mean_sqrt,
                                 rep(0, 2432))

# Create single column with first fitness values (ABSOLUTE VALUES), then FFD values:
datadef.stack2$fitness.FFD.stack <- c(round(subset(datadef_total,
                                                   !is.na(shoot_vol_mean))$mean_fitness_fl),
                                      subset(datadef,!is.na(FFD)&
                                               id %in%  subset(datadef_total,
                                                               !is.na(shoot_vol_mean))$id)$FFD)

# Create 3 index columns needed for MCMCglmm
datadef.stack2$traits <- as.factor(c(rep("fitness", 791), rep("FFD", 2432)))
datadef.stack2$variable <- as.factor(datadef.stack2$traits)
# Fitness will be modelled with an overdispersed Poisson 
# FFD will be modelled with a Gaussian distribution
# Specify this with the column 'family':
datadef.stack2$family <- c(rep("poisson", 791), rep("gaussian", 2432))
datadef.stack2 <- data.frame(datadef.stack2)

datadef.stack2$id <- as.factor(datadef.stack2$id)
datadef.stack2$year <- as.factor(datadef.stack2$year)
head(datadef.stack2)
```

```{r eval=FALSE, include=TRUE}
bivar2.all <- MCMCglmm(fitness.FFD.stack ~ variable - 1 +
                         # ^ means for each variable 
                         # (and no overall mean (hence "-1"))
                         at.level(variable, "FFD"):temp +  
                       # single fixed effect of temp
                         at.level(variable,"fitness"):cn_shoot_vol,
                       random = ~us(at.level(variable, "FFD")):year +
                         us(at.level(variable, "FFD") + 
                              at.level(variable,"FFD"):temp):id,
                       # ^ random intercepts for individual, 
                       # and random slopes for temp|id
                       rcov = ~us(at.level(variable, "fitness")):id + 
                         # ^ variance between indivdiuals in fitness
                         # (which is residual variance)
                         us(at.level(variable, "FFD")):Obs,    
                         # ^ residual variance within indivdiuals between years 
                       # (labelled by 'Obs')
                       data = datadef.stack2,
                       prior = priorBiv, 
                       family = NULL, # specified already in the data-set
                       nitt = 2100 * sc, thin = sc, burnin = 100 * sc, 
                       verbose = F,singular.ok = T) 
# nitt = burnin + thin*(n samples to keep)
# Aim to store 2000 iterations
```

```{r}
kable(summary(bivar2.all)$solutions,digits=c(3,3,3,0,3),caption="Fixed effects")
kable(summary(bivar2.all)$Gcovariances,digits=c(3,3,3,0),caption="Random effects")
kable(summary(bivar2.all)$Rcovariances,digits=c(3,3,3,0),caption="Random effects")
kable(diag(autocorr(bivar2.all$Sol)[2, , ]),caption="Autocorrelation")
kable(diag(autocorr(bivar2.all$VCV)[2, , ]),caption="Autocorrelation")
```

Among-individual correlation between intercepts and slopes for FFD, between FFD and fitness and between fitness and variation in slopes for FFD:

```{r fig.height=2.5, fig.width=6}
cor_bivar2.all_intslope <- 
  bivar2.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\").id"]/
(sqrt(bivar2.all$VCV[,"at.level(variable, \"FFD\").id:at.level(variable, \"FFD\").id"])*
sqrt(bivar2.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\"):temp.id"]))
posterior.mode(cor_bivar2.all_intslope)
HPDinterval(cor_bivar2.all_intslope)

cor_bivar2.all_intfit <-
  bivar2.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"FFD\").id"]/
  (sqrt(bivar2.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"fitness\").id"])*
     sqrt(bivar2.all$VCV[,"at.level(variable, \"FFD\").id:at.level(variable, \"FFD\").id"]))
posterior.mode(cor_bivar2.all_intfit)
HPDinterval(cor_bivar2.all_intfit)

cor_bivar2.all_slopefit <- 
  bivar2.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"FFD\"):temp.id"]/
  (sqrt(bivar2.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"fitness\").id"])*
     sqrt(bivar2.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\"):temp.id"]))
posterior.mode(cor_bivar2.all_slopefit)
HPDinterval(cor_bivar2.all_slopefit)
```

Similar results as in model without shoot volume.

##### Extract selection coefficients

```{r fig.height=2.5, fig.width=6}
# Extract 3x3 matrix of variance-covariance values for intercepts and slopes 
# of temp, and fitness 
# These are in the 2nd-10th columns of model output
P.bivar2.all<- bivar2.all$VCV[,2:10]         
P.bivar2.all.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.bivar2.all.mode[k] <- posterior.mode(P.bivar2.all
                                                    [,k])
P.bivar2.all.mode

# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.bivar2.all <- bivar2.all$VCV[, c(4,7)]
S.bivar2.all <- P.bivar2.all[, c(3,6)] # This is exactly the same as above
colnames(S.bivar2.all) <- c("S_intercepts", "S_slopes")
S.bivar2.all.mode <- P.bivar2.all.mode[1:2, 3]
S.bivar2.all.mode
posterior.mode(mcmc(S.bivar2.all)) # This is exactly the same as above
HPDinterval(mcmc(S.bivar2.all))

# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
n <- length(bivar2.all$VCV[,2])   # sample size
beta_post_bivar2.all <- matrix(NA, n ,2)

for (i in 1:n) {
  P3 <- matrix(rep(NA, 9), nrow = 3)  
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3[k] <- P.bivar2.all[i, k] }  
  P2 <- P3[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S <- P3[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_bivar2.all[i,] <- solve(P2) %*% S   # selection gradients beta = P^-1 * S
}

# Finally, extract and plot the selection gradients posterior modes 
# and 95% credible intervals for both selection on intercepts (trait value) 
# and slopes (trait plasticity).

colnames(beta_post_bivar2.all) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_bivar2.all))
HPDinterval(mcmc(beta_post_bivar2.all))
```

Selection coefficients give similar results as in model without shoot volume.

### 3. mean_fitness_study, no condition variable

Stack data:

```{r}
# Create a single data-set "datadef.stack3", with single column at start 
# to index observations
datadef.stack3 <- c()
datadef.stack3$Obs <- 1:(837 + 2478) 
datadef.stack3$id <- c(as.character(datadef_total$id),
                      as.character(subset(datadef,!is.na(FFD))$id)) 
# ids in alphabetical order

# Year column is only relevant for FFD, but is set to first_yr for fitness values
datadef.stack3$year <- c(datadef_total$first_yr,
                         subset(datadef,!is.na(FFD))$year)

# Temperature column is only relevant for FFD, but is set to 0 for fitness values
datadef.stack3$temp <- c(rep(0, 837), subset(datadef,!is.na(FFD))$cmean_4)

# Create single column with first fitness values (ABSOLUTE VALUES), then FFD values:
datadef.stack3$fitness.FFD.stack <- c(round(datadef_total$mean_fitness_study),
                                      subset(datadef,!is.na(FFD))$FFD)

# Create 3 index columns needed for MCMCglmm
datadef.stack3$traits <- as.factor(c(rep("fitness", 837), rep("FFD", 2478)))
datadef.stack3$variable <- as.factor(datadef.stack3$traits)
# Fitness will be modelled with an overdispersed Poisson 
# FFD will be modelled with a Gaussian distribution
# Specify this with the column 'family':
datadef.stack3$family <- c(rep("poisson", 837), rep("gaussian", 2478))
datadef.stack3 <- data.frame(datadef.stack3)

datadef.stack3$id <- as.factor(datadef.stack3$id)
datadef.stack3$year <- as.factor(datadef.stack3$year)
head(datadef.stack3)
```

```{r eval=FALSE, include=TRUE}
bivar3.all <- MCMCglmm(fitness.FFD.stack ~ variable - 1 +
                         # ^ means for each variable 
                         # (and no overall mean (hence "-1"))
                         at.level(variable, "FFD"):temp,  
                       # single fixed effect of temp
                       random = ~us(at.level(variable, "FFD")):year +
                         us(at.level(variable, "FFD") + 
                              at.level(variable,"FFD"):temp):id,
                       # ^ random intercepts for individual, 
                       # and random slopes for temp|id
                       rcov = ~us(at.level(variable, "fitness")):id + 
                         # ^ variance between indivdiuals in fitness
                         # (which is residual variance)
                         us(at.level(variable, "FFD")):Obs,    
                         # ^ residual variance within indivdiuals between years 
                       # (labelled by 'Obs')
                       data = datadef.stack3,
                       prior = priorBiv, 
                       family = NULL, # specified already in the data-set
                       nitt = 2100 * sc, thin = sc, burnin = 100 * sc, 
                       verbose = F,singular.ok = T) 
# nitt = burnin + thin*(n samples to keep)
# Aim to store 2000 iterations
```

```{r}
kable(summary(bivar3.all)$solutions,digits=c(3,3,3,0,3),caption="Fixed effects")
kable(summary(bivar3.all)$Gcovariances,digits=c(3,3,3,0),caption="Random effects")
kable(summary(bivar3.all)$Rcovariances,digits=c(3,3,3,0),caption="Random effects")
kable(diag(autocorr(bivar3.all$Sol)[2, , ]),caption="Autocorrelation")
kable(diag(autocorr(bivar3.all$VCV)[2, , ]),caption="Autocorrelation")
```

Among-individual correlation between intercepts and slopes for FFD, between FFD and fitness and between fitness and variation in slopes for FFD:

```{r fig.height=2.5, fig.width=6}
cor_bivar3.all_intslope <- 
  bivar3.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\").id"]/
(sqrt(bivar3.all$VCV[,"at.level(variable, \"FFD\").id:at.level(variable, \"FFD\").id"])*
sqrt(bivar3.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\"):temp.id"]))
posterior.mode(cor_bivar3.all_intslope)
HPDinterval(cor_bivar3.all_intslope)

cor_bivar3.all_intfit <-
  bivar3.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"FFD\").id"]/
  (sqrt(bivar3.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"fitness\").id"])*
     sqrt(bivar3.all$VCV[,"at.level(variable, \"FFD\").id:at.level(variable, \"FFD\").id"]))
posterior.mode(cor_bivar3.all_intfit)
HPDinterval(cor_bivar3.all_intfit)

cor_bivar3.all_slopefit <- 
  bivar3.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"FFD\"):temp.id"]/
  (sqrt(bivar3.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"fitness\").id"])*
     sqrt(bivar3.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\"):temp.id"]))
posterior.mode(cor_bivar3.all_slopefit)
HPDinterval(cor_bivar3.all_slopefit)
```

Similar results as in previous models.

##### Extract selection coefficients

```{r fig.height=2.5, fig.width=6}
# Extract 3x3 matrix of variance-covariance values for intercepts and slopes 
# of temp, and fitness 
# These are in the 2nd-10th columns of model output
P.bivar3.all<- bivar3.all$VCV[,2:10]         
P.bivar3.all.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.bivar3.all.mode[k] <- posterior.mode(P.bivar3.all
                                                    [,k])
P.bivar3.all.mode

# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.bivar3.all <- bivar3.all$VCV[, c(4,7)]
S.bivar3.all <- P.bivar3.all[, c(3,6)] # This is exactly the same as above
colnames(S.bivar3.all) <- c("S_intercepts", "S_slopes")
S.bivar3.all.mode <- P.bivar3.all.mode[1:2, 3]
S.bivar3.all.mode
posterior.mode(mcmc(S.bivar3.all)) # This is exactly the same as above
HPDinterval(mcmc(S.bivar3.all))

# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
n <- length(bivar3.all$VCV[,2])   # sample size
beta_post_bivar3.all <- matrix(NA, n ,2)

for (i in 1:n) {
  P3 <- matrix(rep(NA, 9), nrow = 3)  
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3[k] <- P.bivar3.all[i, k] }  
  P2 <- P3[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S <- P3[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_bivar3.all[i,] <- solve(P2) %*% S   # selection gradients beta = P^-1 * S
}

# Finally, extract and plot the selection gradients posterior modes 
# and 95% credible intervals for both selection on intercepts (trait value) 
# and slopes (trait plasticity).

colnames(beta_post_bivar3.all) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_bivar3.all))
HPDinterval(mcmc(beta_post_bivar3.all))
```

The selection differentials are "significant" for RN intercept (negative) but not for slope. The selection gradients are significant for RN intercept (negative) but not for RN slope. This means that there is significant total and direct selection on the intercept of the RN.

### 4. mean_fitness_study, with shoot volume

Stack data:

```{r}
# Create a single data-set "datadef.stack4", with single column at start 
# to index observations
datadef.stack4 <- c()
nrow(subset(datadef_total,!is.na(shoot_vol_mean))) # 791 ids with info on shoot_vol

nrow(subset(datadef,!is.na(FFD)&
              id %in% subset(datadef_total,!is.na(shoot_vol_mean))$id)) 
# 2432 cases with info on FFD and fitness 
# corresponding to the 791 ids with info on shoot_vol

# Check that those cases correspond to those 791 individuals
length(unique(subset(datadef,!is.na(FFD)&
         id %in% subset(datadef_total,!is.na(shoot_vol_mean))$id)$id))

datadef.stack4$Obs <- 1:(791 + 2432) 
datadef.stack4$id <- c(as.character(subset(datadef_total,!is.na(shoot_vol_mean))$id),
                      as.character(subset(datadef,!is.na(FFD)&
                            id %in% subset(datadef_total,!is.na(shoot_vol_mean))$id)$id)) 
# ids in alphabetical order

# Year column is only relevant for FFD, but is set to first_yr for fitness values
datadef.stack4$year <- c(subset(datadef_total,!is.na(shoot_vol_mean))$first_yr,
                         subset(datadef,!is.na(FFD)&
                                  id %in%subset(datadef_total,
                                                !is.na(shoot_vol_mean))$id)$year)

# Temperature column is only relevant for FFD, but is set to 0 for fitness values
datadef.stack4$temp <- c(rep(0, 791), 
                         subset(datadef,!is.na(FFD)&
                                  id %in%subset(datadef_total,
                                                !is.na(shoot_vol_mean))$id)$cmean_4)

# Shoot volume column is only relevant for fitness, but is set to 0 for FFD values
# Using sqrt of mean shoot volume over all years when available, centered
datadef.stack4$cn_shoot_vol <- c(subset(datadef_total,
                                        !is.na(shoot_vol_mean))$cn_shoot_vol_mean_sqrt,
                                 rep(0, 2432))

# Create single column with first fitness values (ABSOLUTE VALUES), then FFD values:
datadef.stack4$fitness.FFD.stack <- c(round(subset(datadef_total,
                                                   !is.na(shoot_vol_mean))$mean_fitness_study),
                                      subset(datadef,!is.na(FFD)&
                                               id %in%  subset(datadef_total,
                                                               !is.na(shoot_vol_mean))$id)$FFD)

# Create 3 index columns needed for MCMCglmm
datadef.stack4$traits <- as.factor(c(rep("fitness", 791), rep("FFD", 2432)))
datadef.stack4$variable <- as.factor(datadef.stack4$traits)
# Fitness will be modelled with an overdispersed Poisson 
# FFD will be modelled with a Gaussian distribution
# Specify this with the column 'family':
datadef.stack4$family <- c(rep("poisson", 791), rep("gaussian", 2432))
datadef.stack4 <- data.frame(datadef.stack4)

datadef.stack4$id <- as.factor(datadef.stack4$id)
datadef.stack4$year <- as.factor(datadef.stack4$year)
head(datadef.stack4)
```

```{r eval=FALSE, include=TRUE}
bivar4.all <- MCMCglmm(fitness.FFD.stack ~ variable - 1 +
                         # ^ means for each variable 
                         # (and no overall mean (hence "-1"))
                         at.level(variable, "FFD"):temp +  
                       # single fixed effect of temp
                         at.level(variable,"fitness"):cn_shoot_vol,
                       random = ~us(at.level(variable, "FFD")):year +
                         us(at.level(variable, "FFD") + 
                              at.level(variable,"FFD"):temp):id,
                       # ^ random intercepts for individual, 
                       # and random slopes for temp|id
                       rcov = ~us(at.level(variable, "fitness")):id + 
                         # ^ variance between indivdiuals in fitness
                         # (which is residual variance)
                         us(at.level(variable, "FFD")):Obs,    
                         # ^ residual variance within indivdiuals between years 
                       # (labelled by 'Obs')
                       data = datadef.stack4,
                       prior = priorBiv, 
                       family = NULL, # specified already in the data-set
                       nitt = 2100 * sc, thin = sc, burnin = 100 * sc, 
                       verbose = F,singular.ok = T) 
# nitt = burnin + thin*(n samples to keep)
# Aim to store 2000 iterations
```

```{r}
kable(summary(bivar4.all)$solutions,digits=c(3,3,3,0,3),caption="Fixed effects")
kable(summary(bivar4.all)$Gcovariances,digits=c(3,3,3,0),caption="Random effects")
kable(summary(bivar4.all)$Rcovariances,digits=c(3,3,3,0),caption="Random effects")
kable(diag(autocorr(bivar4.all$Sol)[2, , ]),caption="Autocorrelation")
kable(diag(autocorr(bivar4.all$VCV)[2, , ]),caption="Autocorrelation")
```

Among-individual correlation between intercepts and slopes for FFD, between FFD and fitness and between fitness and variation in slopes for FFD:

```{r fig.height=2.5, fig.width=6}
cor_bivar4.all_intslope <- 
  bivar4.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\").id"]/
(sqrt(bivar4.all$VCV[,"at.level(variable, \"FFD\").id:at.level(variable, \"FFD\").id"])*
sqrt(bivar4.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\"):temp.id"]))
posterior.mode(cor_bivar4.all_intslope)
HPDinterval(cor_bivar4.all_intslope)

cor_bivar4.all_intfit <-
  bivar4.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"FFD\").id"]/
  (sqrt(bivar4.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"fitness\").id"])*
     sqrt(bivar4.all$VCV[,"at.level(variable, \"FFD\").id:at.level(variable, \"FFD\").id"]))
posterior.mode(cor_bivar4.all_intfit)
HPDinterval(cor_bivar4.all_intfit)

cor_bivar4.all_slopefit <- 
  bivar4.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"FFD\"):temp.id"]/
  (sqrt(bivar4.all$VCV[,"at.level(variable, \"fitness\").id:at.level(variable, \"fitness\").id"])*
     sqrt(bivar4.all$VCV[,"at.level(variable, \"FFD\"):temp.id:at.level(variable, \"FFD\"):temp.id"]))
posterior.mode(cor_bivar4.all_slopefit)
HPDinterval(cor_bivar4.all_slopefit)
```

Similar results as in previous models.

##### Extract selection coefficients

```{r fig.height=2.5, fig.width=6}
# Extract 3x3 matrix of variance-covariance values for intercepts and slopes 
# of temp, and fitness 
# These are in the 2nd-10th columns of model output
P.bivar4.all<- bivar4.all$VCV[,2:10]         
P.bivar4.all.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.bivar4.all.mode[k] <- posterior.mode(P.bivar4.all
                                                    [,k])
P.bivar4.all.mode

# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.bivar4.all <- bivar4.all$VCV[, c(4,7)]
S.bivar4.all <- P.bivar4.all[, c(3,6)] # This is exactly the same as above
colnames(S.bivar4.all) <- c("S_intercepts", "S_slopes")
S.bivar4.all.mode <- P.bivar4.all.mode[1:2, 3]
S.bivar4.all.mode
posterior.mode(mcmc(S.bivar4.all)) # This is exactly the same as above
HPDinterval(mcmc(S.bivar4.all))

# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
n <- length(bivar4.all$VCV[,2])   # sample size
beta_post_bivar4.all <- matrix(NA, n ,2)

for (i in 1:n) {
  P3 <- matrix(rep(NA, 9), nrow = 3)  
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3[k] <- P.bivar4.all[i, k] }  
  P2 <- P3[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S <- P3[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_bivar4.all[i,] <- solve(P2) %*% S   # selection gradients beta = P^-1 * S
}

# Finally, extract and plot the selection gradients posterior modes 
# and 95% credible intervals for both selection on intercepts (trait value) 
# and slopes (trait plasticity).

colnames(beta_post_bivar4.all) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_bivar4.all))
HPDinterval(mcmc(beta_post_bivar4.all))
```

The selection differentials are "significant" for RN intercept (negative), but not for RN slope. The selection gradients are significant for both RN intercept (negative) and slope (positive).

# B) brms models

## Univariate models

### FFD with random effect of year only

```{r}
my.cores <- detectCores()
```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
univar.FFD_yearonly.all.brm<-brm(formula=FFD~cmean_4+(1|year),data=datadef,
                warmup = 1000,iter = 4000,thin=2,chains=4,
                # 4 chains, each with 4000 iterations
                inits = "random",seed = 12345,cores = my.cores)
# Total of 6000 post-warmup samples
```

```{r}
summary(univar.FFD_yearonly.all.brm)
```

### FFD with random effects of year and individual-intercept

```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
univar.FFD.all.brm<-brm(formula=FFD~cmean_4+(1|year)+(1|id),data=datadef,
                warmup = 1000,iter = 4000,thin=2,chains=4,
                # 4 chains, each with 4000 iterations
                inits = "random",seed = 12345,cores = my.cores)
# Total of 6000 post-warmup samples
```

```{r }
summary(univar.FFD.all.brm)
```

### Random regression for FFD, including random effects of individual slopes and covariance between intercept and slope

```{r, eval=FALSE, include=TRUE}
univar.FFD_RR.all.brm<-brm(formula=FFD~cmean_4+(1|year)+(cmean_4|id),
                           data=datadef,
                           warmup = 1000,iter = 4000,thin=2,chains=4,
                           inits = "random",seed = 12345,cores = my.cores,
                           sample_prior="yes")
# Total of 6000 post-warmup samples
```

```{r}
summary(univar.FFD_RR.all.brm) 
```

### Compare models

```{r eval=FALSE, include=FALSE}
univar.FFD_yearonly.all.brm <- add_criterion(univar.FFD_yearonly.all.brm,"loo")
univar.FFD.all.brm <- add_criterion(univar.FFD.all.brm,"loo")
univar.FFD_RR.all.brm <- add_criterion(univar.FFD_RR.all.brm,"loo")
```

```{r}
loo_comp<-loo_compare(univar.FFD_yearonly.all.brm,univar.FFD.all.brm,
                      univar.FFD_RR.all.brm, criterion="loo")
```

```{r}
loo_comp
```

Random regression model is better

### Model evaluation

From https://biol609.github.io/lectures/23c_brms_prediction.html#24_evaluating_brms_models

```{r fig.height=8, fig.width=6}
plot(univar.FFD_yearonly.all.brm) 
plot(univar.FFD.all.brm) 
plot(univar.FFD_RR.all.brm) 
```

Rhat (potential scale reduction statistic): monitors whether a chain has converged to the equilibrium distribution by comparing its behavior to other randomly initialized chains.

-   light: below 1.05 (good)

-   mid: between 1.05 and 1.1 (ok)

-   dark: above 1.1 (too high)

```{r}
mcmc_rhat_hist(rhat(univar.FFD_yearonly.all.brm))+theme_bw()
mcmc_rhat_hist(rhat(univar.FFD.all.brm))+theme_bw()
mcmc_rhat_hist(rhat(univar.FFD_RR.all.brm))+theme_bw()
```

Effective sample size: an estimate of the number of independent draws from the posterior distribution of the estimand of interest.

-   light: between 0.5 and 1 (high)

-   mid: between 0.1 and 0.5 (good)

-   dark: below 0.1 (low)

We should worry about any values less than 0.1.

```{r}
mcmc_neff_hist(neff_ratio(univar.FFD_yearonly.all.brm))+theme_bw()
mcmc_neff_hist(neff_ratio(univar.FFD.all.brm))+theme_bw()
mcmc_neff_hist(neff_ratio(univar.FFD_RR.all.brm))+theme_bw()
```

Autocorrelation: Takes very long / gets stuck, see later.

```{r}
# mcmc_acf(univar.FFD_RR.all.brm,lags=20)
```

Assessing fit: 

Posterior predictive checks:  Compares observed data to simulated data from the posterior predictive distribution (if a model is a good fit, we should be able to use it to generate data that resemble the data that we observed).

https://cran.r-project.org/web/packages/bayesplot/vignettes/graphical-ppcs.html#:~:text=MCMC%20diagnostics%20vignette.-,Graphical%20posterior%20predictive%20checks%20(PPCs),Gabry%20et%20al%2C%202019).

```{r}
y1<-subset(datadef,!is.na(FFD))$FFD # vector of outcome values
yrep1<-posterior_predict(univar.FFD_yearonly.all.brm, draws = 500)
yrep2<-posterior_predict(univar.FFD.all.brm, draws = 500)
yrep3<-posterior_predict(univar.FFD_RR.all.brm, draws = 500)
# matrices of draws from the posterior predictive distribution
```

Each row of the matrix is a draw from the posterior predictive distribution, i.e. a vector with one element for each of the data points in y.

Comparison of the distribution of y and the distributions of the simulated datasets in the yrep matrix

```{r}
ppc_dens_overlay(y1, yrep1[1:500,])
ppc_dens_overlay(y1, yrep2[1:500,])
ppc_dens_overlay(y1, yrep3[1:500,])
```

Separate histograms of y and some of the yrep datasets

```{r}
ppc_hist(y1, yrep1[1:8, ])
ppc_hist(y1, yrep2[1:8, ])
ppc_hist(y1, yrep3[1:8, ])
```

Is this good enough?

```{r message=FALSE, warning=FALSE}
ppc_stat(y1,yrep1,stat="median")
ppc_stat(y1,yrep2,stat="median")
ppc_stat(y1,yrep3,stat="median")
ppc_stat(y1,yrep1,stat="mean")
ppc_stat(y1,yrep2,stat="mean")
ppc_stat(y1,yrep3,stat="mean")
ppc_stat(y1,yrep1,stat="sd")
ppc_stat(y1,yrep2,stat="sd")
ppc_stat(y1,yrep3,stat="sd")
ppc_stat(y1,yrep1,stat="max")
ppc_stat(y1,yrep2,stat="max")
ppc_stat(y1,yrep3,stat="max")
ppc_stat(y1,yrep1,stat="min")
ppc_stat(y1,yrep2,stat="min")
ppc_stat(y1,yrep3,stat="min")
```

This looks quite OK (worst is for the min).

Measure of fit: Bayesian R2, which looks at the model expected variance / (expected variance + residual variance).

```{r}
bayes_R2(univar.FFD_yearonly.all.brm)
bayes_R2(univar.FFD.all.brm)
bayes_R2(univar.FFD_RR.all.brm)
```

Leave-one-out cross validation (LOO): 

From https://www.monicaalexander.com/posts/2020-28-02-bayes_viz/

We are interested in estimating the out-of-sample predictive accuracy at each point i, when all we have to fit the model is data that without point i. We want to estimate the leave-one-out (LOO) posterior predictive densities and a summary of these across all points, which is called the LOO expected log pointwise predictive density. The bigger the numbers, the better we are at predicting the left out point i.

The output mentions Pareto k estimates, which give an indication of how 'influential' each point is. The higher the value of k, the more influential the point. Values of k over 0.7 are not good, and suggest the need for model re-specification. 

```{r eval=FALSE, include=FALSE}
loo1 <- loo(univar.FFD_RR.all.brm, save_psis = TRUE)
# Not sure what the warming means
```

```{r}
plot(loo1)
```

We have 3 obs with k > 0.7 - how bad is this?

Is this OK?

### Extract BLUPs from random regression model

Code was checked by Piet.

```{r}
BLUPs_MCMC.all.brms  <- cbind(as.factor(c(1:837)),
                              as.data.frame(ranef(univar.FFD_RR.all.brm)$id)
                              [c(1,5)])
colnames(BLUPs_MCMC.all.brms) <- c("id", "intercept", "slope")
with(BLUPs_MCMC.all.brms,cor(intercept,slope)) # highly correlated!
```

Plots with BLUPs

```{r, echo=FALSE, fig.height=4, fig.width=10}
ggplot(BLUPs_MCMC.all.brms, aes(id, intercept)) + 
  geom_point(aes(group = id, colour = id), size = 4, alpha = 0.5) + 
  ylab("BLUP intercept estimate") +
  geom_hline(yintercept = 0, lty = 2) + my_theme() +
  theme(axis.text.x = element_blank())
```

```{r, echo=FALSE, fig.height=4, fig.width=10}
ggplot(BLUPs_MCMC.all.brms, aes(id, slope)) + 
  geom_point(aes(group = id, colour = id), size = 4, alpha = 0.5) + 
  ylab("Plasticity (BLUP slope estimate)") +
  geom_hline(yintercept = 0, lty = 2) + my_theme() +
  theme(axis.text.x = element_blank())
```

```{r, echo=FALSE, fig.height=4, fig.width=10}
BLUPs_MCMC.all.brms$id_ordered <- factor(BLUPs_MCMC.all.brms$id,
                                    levels = BLUPs_MCMC.all.brms$id[order(BLUPs_MCMC.all.brms$slope)])
ggplot(BLUPs_MCMC.all.brms, aes(id_ordered, slope)) +
  geom_bar(stat = "identity", aes(group = id, fill = id)) +
  xlab("Id (in ranked order of plasticity)") +
  ylab("Plasticity (BLUP slope estimate)") +
  my_theme() + theme(axis.text.x = element_blank())
```

## Bivariate models

### 1. mean_fitness_fl, no condition variable

Using the ID-syntax to specify fitness to be correlated with the intercept and slope of FFD on temperature - code has been checked by Piet.

Regarding distributions, I tried Poisson distribution for fitness, but not sure how eventual overdispersion is handled. I also tried adding an observation-level random effect, and using a negative binomial distribution. Results seem quite similar.

#### Poisson distribution

```{r message=FALSE, warning=FALSE}
datadef<-left_join(datadef,datadef_total[c(1:3,9)]) 
# Add info on mean fitness and mean shoot volume
bf_FFD <- bf(FFD ~ cmean_4 + (cmean_4|ID1|id) + (1|year)) # Set up model formula
bf_fitness <- bf(round(mean_fitness_fl) ~  (1|ID1|id)) # Set up model formula
# Specifying group-level effects of the same grouping factor (id here) 
# to be correlated across formulas
# Expand the | operator into |<ID>|, where <ID> can be any value (ID1 here)
# Group-level terms with the same ID1 will be modeled as correlated 
# if they share same grouping factor(s)
```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
bivar1.all.brm.pois<-brm(bf_FFD + bf_fitness, family = c(gaussian, poisson), 
                         data = datadef,
                         warmup = 1000,iter = 4000,chains=4,thin=2,
                         inits = "random",seed = 12345,cores = my.cores,
                         save_all_pars=TRUE)
# Total of 6000 post-warmup samples
```

```{r}
summary(bivar1.all.brm.pois)
```

#### Negative binomial distribution

```{r, eval=FALSE, include=TRUE}
bivar1.all.brm.nb<-brm(bf_FFD + bf_fitness, family = c(gaussian, negbinomial), 
                       data = datadef,warmup = 1000,iter = 4000,chains=4,thin=2,
                         inits = "random",seed = 12345,cores = my.cores,
                       save_all_pars=TRUE)
```

```{r}
summary(bivar1.all.brm.nb)
```

#### Model evaluation: Compare models

```{r fig.height=8, fig.width=6}
plot(bivar1.all.brm.pois) 
plot(bivar1.all.brm.nb)
```

Rhat

```{r}
mcmc_rhat_hist(rhat(bivar1.all.brm.pois))+theme_bw()
mcmc_rhat_hist(rhat(bivar1.all.brm.nb))+theme_bw()
```

Effective sample size: 

```{r}
mcmc_neff_hist(neff_ratio(bivar1.all.brm.pois))+theme_bw()
mcmc_neff_hist(neff_ratio(bivar1.all.brm.nb))+theme_bw()
```

Posterior predictive checks:  

```{r}
y2_fitness<-round(subset(datadef,!is.na(FFD))$mean_fitness_fl)
y2_FFD<-subset(datadef,!is.na(FFD))$FFD # vectors of outcome values
yrep2_fitness_pois<-posterior_predict(bivar1.all.brm.pois, 
                                 draws = 500,resp="roundmeanfitnessfl")
yrep2_FFD_pois<-posterior_predict(bivar1.all.brm.pois, 
                                 draws = 500,resp="FFD")
yrep2_fitness_nb<-posterior_predict(bivar1.all.brm.nb, 
                                 draws = 500,resp="roundmeanfitnessfl")
yrep2_FFD_nb<-posterior_predict(bivar1.all.brm.nb, 
                                 draws = 500,resp="FFD")
# matrices of draws from the posterior predictive distribution
```

Each row of the matrix is a draw from the posterior predictive distribution, i.e. a vector with one element for each of the data points in y.

Comparison of the distribution of y and the distributions of the simulated datasets in the yrep matrix

```{r}
ppc_dens_overlay(y2_fitness, yrep2_fitness_pois[1:500,])
ppc_dens_overlay(y2_fitness, yrep2_fitness_pois[1:500,])+xlim(0, 10)
ppc_dens_overlay(y2_fitness, yrep2_fitness_nb[1:500,])
ppc_dens_overlay(y2_fitness, yrep2_fitness_nb[1:500,])+xlim(0,10)
ppc_dens_overlay(y2_FFD, yrep2_FFD_pois[1:500,])
ppc_dens_overlay(y2_FFD, yrep2_FFD_nb[1:500,])
```

Poisson and negative binomial look pretty similar.

Separate histograms of y and some of the yrep datasets

```{r}
ppc_hist(y2_fitness, yrep2_fitness_pois[1:8, ])
ppc_hist(y2_fitness, yrep2_fitness_pois[1:8, ])+
  coord_cartesian(xlim = c(-1, 20))
ppc_hist(y2_fitness, yrep2_fitness_nb[1:8, ])
ppc_hist(y2_fitness, yrep2_fitness_nb[1:8, ])+
  coord_cartesian(xlim = c(-1, 20))
```

Here, negative binomial looks a bit better for fitness.

```{r message=FALSE, warning=FALSE}
ppc_stat(y2_fitness, yrep2_fitness_pois,stat="median")
ppc_stat(y2_fitness, yrep2_fitness_nb,stat="median")
ppc_stat(y2_fitness, yrep2_fitness_pois,stat="mean")
ppc_stat(y2_fitness, yrep2_fitness_nb,stat="mean")
ppc_stat(y2_fitness, yrep2_fitness_pois,stat="sd")
ppc_stat(y2_fitness, yrep2_fitness_nb,stat="sd")
ppc_stat(y2_fitness, yrep2_fitness_pois,stat="max")
ppc_stat(y2_fitness, yrep2_fitness_nb,stat="max")
```

Look at the distribution of the proportion of zeros over the replicated datasets from the posterior predictive distribution in yrep and compare to the proportion of observed zeros in y.

```{r}
# Define a function that takes a vector as input
# and returns the proportion of zeros:
prop_zero <- function(x) mean(x == 0)
prop_zero(y2_fitness) # check proportion of zeros in y
```

```{r}
ppc_stat(y2_fitness, yrep2_fitness_pois, stat = "prop_zero", binwidth = 0.005)
ppc_stat(y2_fitness, yrep2_fitness_nb, stat = "prop_zero", binwidth = 0.005)
```

They look quite similar.

Measure of fit: Bayesian R2

```{r}
bayes_R2(bivar1.all.brm.pois)
bayes_R2(bivar1.all.brm.nb)
```

Very similar.

Widely Applicable Information Criterion (WAIC):

```{r}
waic1<-waic(bivar1.all.brm.pois,bivar1.all.brm.nb,compare=T)
```

```{r}
waic1
```

Suggests that poisson is a bit better, but not sure we can trust this.

Leave-one-out cross validation (LOO): 

```{r eval=FALSE, include=FALSE}
loo2_pois <- loo(bivar1.all.brm.pois, save_psis = TRUE, moment_match = TRUE)
loo2_nb <- loo(bivar1.all.brm.nb, save_psis = TRUE, moment_match = TRUE)
```

Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.
Found 263 observations with a pareto_k > 0.7 in model 'bivar1.all.brm.pois'. With this many problematic observations, it may be more appropriate to use 'kfold' with argument 'K = 10' to perform 10-fold cross-validation rather than LOO.

```{r}
loo_compare(loo2_pois,loo2_nb)
```

Suggests that poisson is a bit better, but not sure we can trust this.

```{r}
plot(loo2_pois)
plot(loo2_nb)
```

LOO predictive checks

```{r}
ppc_loo_pit_overlay(y2_fitness, yrep2_fitness_pois, 
                    lw = weights(loo2_pois$psis_object))
ppc_loo_pit_qq(y2_fitness, yrep2_fitness_pois, 
                    lw = weights(loo2_pois$psis_object))
ppc_loo_pit_qq(y2_fitness, yrep2_fitness_pois, 
                    lw = weights(loo2_pois$psis_object),compare="normal")
ppc_loo_pit_overlay(y2_fitness, yrep2_fitness_nb, 
                    lw = weights(loo2_nb$psis_object))
ppc_loo_pit_qq(y2_fitness, yrep2_fitness_nb, 
                    lw = weights(loo2_nb$psis_object))
ppc_loo_pit_qq(y2_fitness, yrep2_fitness_nb, 
                    lw = weights(loo2_nb$psis_object),compare="normal")
```

All looking quite bad!

k-fold cross-validation (K=5):

```{r eval=FALSE, include=FALSE}
kfold1_pois<-kfold(bivar1.all.brm.pois,K=5,chains=1,cores=my.cores)
kfold1_nb<-kfold(bivar1.all.brm.nb,K=5,chains=1,cores=my.cores)
```

```{r}
loo_compare(kfold1_pois,kfold1_nb)
```

Suggests that poisson is a bit better, and I guess we can trust this one as it gave no warnings?.

#### Prior predictive checks

Not sure what I am doing here...

```{r eval=FALSE, include=FALSE}
prior1<-c(set_prior("normal(0,1000)", class = "b"),
          set_prior("student_t(3, 58.6, 6.6)", class = "Intercept",resp="FFD"),
          set_prior("student_t(3, 1.4, 2.5)",class = "Intercept",
                    resp="roundmeanfitnessfl"))
bivar1.all.brm.pois_priors<-brm(bf_fitness + bf_FFD,
       family = c(poisson, gaussian), data = datadef,
       prior = prior1,
                         warmup = 1000,iter = 4000,chains=4,thin=2,
                         inits = "random",seed = 12345,cores = my.cores,
                         sample_prior="only")
```


```{r}
yrep2_fitness_pois_priors<-posterior_predict(bivar1.all.brm.pois_priors, 
                                 draws = 500,resp="roundmeanfitnessfl")
yrep2_FFD_pois_priors<-posterior_predict(bivar1.all.brm.pois_priors, 
                                 draws = 500,resp="FFD")
# matrices of draws from the posterior(prior??) predictive distribution
```

Comparison of the distribution of y and the distributions of the simulated datasets in the yrep matrix

```{r}
ppc_dens_overlay(y2_fitness, yrep2_fitness_pois_priors[1:500,])
ppc_dens_overlay(y2_fitness, yrep2_fitness_pois_priors[1:500,])+xlim(0, 10)
ppc_dens_overlay(y2_FFD, yrep2_FFD_pois_priors[1:500,])
```

#### Extract selection coefficients

##### Poisson model

Using example code from Piet. I am not sure what I am doing here! The selection differentials and gradients are not so different from those obtained from MCMCglmm models so this is maybe right, but the code would need to be revised.

```{r}
# Extract posterior samples
bivar1.all.brm.pois_post <- posterior_samples(bivar1.all.brm.pois)
bivar1.all.brm.pois_post <- as.mcmc(bivar1.all.brm.pois_post)
#head(bivar1.all.brm.pois_post)[,1:20]

# [,4] sd_id__FFD_Intercept
# [,5] sd_id__FFD_cmean_4
# [,6] sd_id__roundmeanfitnessfl_Intercept 
# [,8] cor_id__FFD_Intercept__FFD_cmean_4
# [,9] cor_id__FFD_Intercept__roundmeanfitnessfl_Intercept
# [,10] cor_id__FFD_cmean_4__roundmeanfitnessfl_Intercept

# Sampling G-matrices from the posterior to calculate intervals estimates
# Use all posterior samples (no need to sample a random subset)
# Result is list with G-matrix
sample.gmat1 <- function(data, replicates = 5000) {
  
  ##Initialize the results list (list of lists)
  foo <- list(gmat = matrix(rep(0,3*3), ncol = 3))
  results.list <- list()
  for(j in 1:replicates) { results.list[[j]] <- foo }
  
  for(i in 1:replicates) {
    diag(results.list[[i]]$gmat) <- data[i,4:6]^2 #Get the diagonal
    
    #Upper diagonal
    results.list[[i]]$gmat[1,2] <- data[i,4]*data[i,5]*data[i,8]
    results.list[[i]]$gmat[1,3] <- data[i,4]*data[i,6]*data[i,9]
    results.list[[i]]$gmat[2,3] <- data[i,5]*data[i,6]*data[i,10]
    
    #Lower diagonal
    results.list[[i]]$gmat[2,1] <- results.list[[i]]$gmat[1,2]
    results.list[[i]]$gmat[3,1] <- results.list[[i]]$gmat[1,3]
    results.list[[i]]$gmat[3,2] <- results.list[[i]]$gmat[2,3]
    
  }
  
  return(results.list)
}

sampled.gmat1 <- sample.gmat1(bivar1.all.brm.pois_post, replicates = 1000) 
sampled.gmat1[[2]]
```

```{r}
sgmat1 <- lapply(sampled.gmat1, `[`, c('gmat')) #Get list 'gmat' from each list
sgmat1 <- unname(sapply(sgmat1, '[[', 1)) #Change to matrix
str(sgmat1)
```


```{r}
sgmat1 <- t(sgmat1)

P.modelBV_RR1 <- sgmat1
P.modelBV_RR1.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.modelBV_RR1.mode[k] <- posterior.mode(mcmc(sgmat1[,k]))
P.modelBV_RR1.mode
```

```{r}
# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.modelBV_RR1 <- sgmat1[,c(3,6)]
colnames(S.modelBV_RR1) <- c("S_intercepts", "S_slopes")
S.modelBV_RR1.mode <- P.modelBV_RR1.mode[1:2, 3]
S.modelBV_RR1.mode
```

```{r}
posterior.mode(mcmc(S.modelBV_RR1))
```

```{r}
HPDinterval(mcmc(S.modelBV_RR1))
```

```{r}
# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
beta_post_RR1 <- matrix(NA, nrow(S.modelBV_RR1) ,2)

for (i in 1:nrow(S.modelBV_RR1)) {
  P3_1 <- matrix(rep(NA, 9), nrow = 3) 
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3_1[k] <- P.modelBV_RR1[i, k] }  
  P2_1 <- P3_1[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S1 <- P3_1[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_RR1[i,] <- solve(P2_1) %*% S1   # selection gradients beta = P^-1 * S
}

colnames(beta_post_RR1) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_RR1))
```

```{r}
HPDinterval(mcmc(beta_post_RR1))
```

##### Negative binomial model

```{r}
# Extract posterior samples
bivar1.all.brm.nb_post <- posterior_samples(bivar1.all.brm.nb)
bivar1.all.brm.nb_post <- as.mcmc(bivar1.all.brm.nb_post)
#head(bivar1.all.brm.nb_post)[,1:20]

# [,4] sd_id__FFD_Intercept
# [,5] sd_id__FFD_cmean_4
# [,6] sd_id__roundmeanfitnessfl_Intercept 
# [,8] cor_id__FFD_Intercept__FFD_cmean_4
# [,9] cor_id__FFD_Intercept__roundmeanfitnessfl_Intercept
# [,10] cor_id__FFD_cmean_4__roundmeanfitnessfl_Intercept

sampled.gmat2 <- sample.gmat1(bivar1.all.brm.nb_post, replicates = 1000) 
sampled.gmat2[[2]]
```

```{r}
sgmat2 <- lapply(sampled.gmat2, `[`, c('gmat')) #Get list 'gmat' from each list
sgmat2 <- unname(sapply(sgmat2, '[[', 1)) #Change to matrix
str(sgmat2)
```


```{r}
sgmat2 <- t(sgmat2)

P.modelBV_RR2 <- sgmat2
P.modelBV_RR2.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.modelBV_RR2.mode[k] <- posterior.mode(mcmc(sgmat2[,k]))
P.modelBV_RR2.mode
```

```{r}
# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.modelBV_RR2 <- sgmat2[,c(3,6)]
colnames(S.modelBV_RR2) <- c("S_intercepts", "S_slopes")
S.modelBV_RR2.mode <- P.modelBV_RR2.mode[1:2, 3]
S.modelBV_RR2.mode
```

```{r}
posterior.mode(mcmc(S.modelBV_RR2))
```

```{r}
HPDinterval(mcmc(S.modelBV_RR2))
```

```{r}
# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
beta_post_RR2 <- matrix(NA, nrow(S.modelBV_RR2) ,2)

for (i in 1:nrow(S.modelBV_RR2)) {
  P3_2 <- matrix(rep(NA, 9), nrow = 3)  
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3_2[k] <- P.modelBV_RR2[i, k] }  
  P2_2 <- P3_2[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S2 <- P3_2[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_RR2[i,] <- solve(P2_2) %*% S2   # selection gradients beta = P^-1 * S
}

colnames(beta_post_RR2) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_RR2))
```

```{r}
HPDinterval(mcmc(beta_post_RR2))
```

### 2. mean_fitness_fl, with shoot volume

```{r message=FALSE, warning=FALSE}
bf_fitness_shoot <- bf(round(mean_fitness_fl) ~  cn_shoot_vol_mean_sqrt +
                         (1|ID1|id)) # Set up model formula
```

#### Poisson distribution

```{r, eval=FALSE, include=TRUE}
bivar2.all.brm.pois<-brm(bf_FFD+bf_fitness_shoot, 
                       family = c(gaussian, poisson), 
                       data = datadef,warmup = 2000,iter = 6000,chains=4,thin=2,
                       inits = "random",seed = 12345,cores = my.cores,
                       control = list(adapt_delta = 0.99, max_treedepth = 15))
```

```{r}
summary(bivar2.all.brm.pois)
```

Due to these warnings, results of bivar2.all.brm.pois are probably not reliable!
Try RERUN with above specifications.

#### Negative binomial distribution

```{r, eval=FALSE, include=TRUE}
bivar2.all.brm.nb<-brm(bf_FFD+bf_fitness_shoot, 
                       family = c(gaussian,negbinomial), 
                       data = datadef,warmup = 1000,iter = 4000,chains=4,thin=2,
                       inits = "random",seed = 12345,cores = my.cores,
                       control = list(adapt_delta = 0.99))
```

```{r}
summary(bivar2.all.brm.nb)
```

No warnings! :)

#### Model evaluation: Compare models

```{r fig.height=8, fig.width=6}
plot(bivar2.all.brm.pois) 
plot(bivar2.all.brm.nb)
```

Rhat

```{r}
mcmc_rhat_hist(rhat(bivar2.all.brm.pois))+theme_bw()
mcmc_rhat_hist(rhat(bivar2.all.brm.nb))+theme_bw()
```

Effective sample size: 
  
```{r}
mcmc_neff_hist(neff_ratio(bivar2.all.brm.pois))+theme_bw()
mcmc_neff_hist(neff_ratio(bivar2.all.brm.nb))+theme_bw()
```

Posterior predictive checks:  
  
```{r}
y3_fitness<-round(subset(datadef,!is.na(FFD)&
                           !is.na(cn_shoot_vol_mean_sqrt))$mean_fitness_fl)
y3_FFD<-subset(datadef,!is.na(FFD)&
                           !is.na(cn_shoot_vol_mean_sqrt))$FFD 
# vectors of outcome values
yrep3_fitness_pois<-posterior_predict(bivar2.all.brm.pois, 
                                      draws = 500,resp="roundmeanfitnessfl")
yrep3_FFD_pois<-posterior_predict(bivar2.all.brm.pois, 
                                  draws = 500,resp="FFD")
yrep3_fitness_nb<-posterior_predict(bivar2.all.brm.nb, 
                                    draws = 500,resp="roundmeanfitnessfl")
yrep3_FFD_nb<-posterior_predict(bivar2.all.brm.nb, 
                                draws = 500,resp="FFD")
# matrices of draws from the posterior predictive distribution
```

Each row of the matrix is a draw from the posterior predictive distribution, i.e. a vector with one element for each of the data points in y.

Comparison of the distribution of y and the distributions of the simulated datasets in the yrep matrix

```{r}
ppc_dens_overlay(y3_fitness, yrep3_fitness_pois[1:500,])
ppc_dens_overlay(y3_fitness, yrep3_fitness_pois[1:500,])+xlim(0, 10)
ppc_dens_overlay(y3_fitness, yrep3_fitness_nb[1:500,])
ppc_dens_overlay(y3_fitness, yrep3_fitness_nb[1:500,])+xlim(0,10)
ppc_dens_overlay(y3_FFD, yrep3_FFD_pois[1:500,])
ppc_dens_overlay(y3_FFD, yrep3_FFD_nb[1:500,])
```

Poisson and negative binomial look pretty similar.

Separate histograms of y and some of the yrep datasets

```{r}
ppc_hist(y3_fitness, yrep3_fitness_pois[1:8, ])
ppc_hist(y3_fitness, yrep3_fitness_pois[1:8, ])+
  coord_cartesian(xlim = c(-1, 20))
ppc_hist(y3_fitness, yrep3_fitness_nb[1:8, ])
ppc_hist(y3_fitness, yrep3_fitness_nb[1:8, ])+
  coord_cartesian(xlim = c(-1, 20))
```

```{r message=FALSE, warning=FALSE}
ppc_stat(y3_fitness, yrep3_fitness_pois,stat="median")
ppc_stat(y3_fitness, yrep3_fitness_nb,stat="median")
ppc_stat(y3_fitness, yrep3_fitness_pois,stat="mean")
ppc_stat(y3_fitness, yrep3_fitness_nb,stat="mean")
ppc_stat(y3_fitness, yrep3_fitness_pois,stat="sd")
ppc_stat(y3_fitness, yrep3_fitness_nb,stat="sd")
ppc_stat(y3_fitness, yrep3_fitness_pois,stat="max")
ppc_stat(y3_fitness, yrep3_fitness_nb,stat="max")
```

Look at the distribution of the proportion of zeros over the replicated datasets from the posterior predictive distribution in yrep and compare to the proportion of observed zeros in y.

```{r}
ppc_stat(y3_fitness, yrep3_fitness_pois, stat = "prop_zero", binwidth = 0.005)
ppc_stat(y3_fitness, yrep3_fitness_nb, stat = "prop_zero", binwidth = 0.005)
```

They look quite similar.

Measure of fit: Bayesian R2

```{r}
bayes_R2(bivar2.all.brm.pois)
bayes_R2(bivar2.all.brm.nb)
```

Very similar.

Leave-one-out cross validation (LOO): (not run)
  
```{r eval=FALSE, include=FALSE}
# loo3_pois <- loo(bivar2.all.brm.pois, save_psis = TRUE, moment_match = TRUE)
# loo3_nb <- loo(bivar2.all.brm.nb, save_psis = TRUE, moment_match = TRUE)
# ```
# 
# ```{r eval=FALSE, include=FALSE}
# save(loo3_pois,file="output/loo3_pois.RData")
# save(loo3_nb,file="output/loo3_nb.RData")
# ```
# 
# ```{r}
# plot(loo3_pois)
# plot(loo3_nb)
# ```
# 
# LOO predictive checks
# 
# ```{r}
# ppc_loo_pit_overlay(y2_fitness, yrep3_fitness_pois, 
#                     lw = weights(loo3_pois$psis_object))
# ppc_loo_pit_qq(y2_fitness, yrep3_fitness_pois, 
#                lw = weights(loo3_pois$psis_object))
# ppc_loo_pit_qq(y2_fitness, yrep3_fitness_pois, 
#                lw = weights(loo3_pois$psis_object),compare="normal")
# ppc_loo_pit_overlay(y2_fitness, yrep3_fitness_nb, 
#                     lw = weights(loo3_nb$psis_object))
# ppc_loo_pit_qq(y2_fitness, yrep3_fitness_nb, 
#                lw = weights(loo3_nb$psis_object))
# ppc_loo_pit_qq(y2_fitness, yrep3_fitness_nb, 
#                lw = weights(loo3_nb$psis_object),compare="normal")
```

#### Extract selection coefficients

##### Poisson model

```{r}
# Extract posterior samples
bivar2.all.brm.pois_post <- posterior_samples(bivar2.all.brm.pois)
bivar2.all.brm.pois_post <- as.mcmc(bivar2.all.brm.pois_post)
#head(bivar2.all.brm.pois_post)[,1:20]

# [,5] sd_id__FFD_Intercept
# [,6] sd_id__FFD_cmean_4
# [,7] sd_id__roundmeanfitnessfl_Intercept 
# [,9] cor_id__FFD_Intercept__FFD_cmean_4
# [,10] cor_id__FFD_Intercept__roundmeanfitnessfl_Intercept
# [,11] cor_id__FFD_cmean_4__roundmeanfitnessfl_Intercept

sample.gmat2 <- function(data, replicates = 5000) {
  
  ##Initialize the results list (list of lists)
  foo <- list(gmat = matrix(rep(0,3*3), ncol = 3))
  results.list <- list()
  for(j in 1:replicates) { results.list[[j]] <- foo }
  
  for(i in 1:replicates) {
    diag(results.list[[i]]$gmat) <- data[i,5:7]^2 #Get the diagonal
    
    #Upper diagonal
    results.list[[i]]$gmat[1,2] <- data[i,5]*data[i,6]*data[i,9]
    results.list[[i]]$gmat[1,3] <- data[i,5]*data[i,7]*data[i,10]
    results.list[[i]]$gmat[2,3] <- data[i,6]*data[i,7]*data[i,11]
    
    #Lower diagonal
    results.list[[i]]$gmat[2,1] <- results.list[[i]]$gmat[1,2]
    results.list[[i]]$gmat[3,1] <- results.list[[i]]$gmat[1,3]
    results.list[[i]]$gmat[3,2] <- results.list[[i]]$gmat[2,3]
    
  }
  
  return(results.list)
}

sampled.gmat3 <- sample.gmat2(bivar2.all.brm.pois_post, replicates = 1000) 

sgmat3 <- lapply(sampled.gmat3, `[`, c('gmat')) #Get list 'gmat' from each list
sgmat3 <- unname(sapply(sgmat3, '[[', 1)) #Change to matrix

sgmat3 <- t(sgmat3)

P.modelBV_RR3 <- sgmat3
P.modelBV_RR3.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.modelBV_RR3.mode[k] <- posterior.mode(mcmc(sgmat3[,k]))

# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.modelBV_RR3 <- sgmat3[,c(3,6)]
colnames(S.modelBV_RR3) <- c("S_intercepts", "S_slopes")
S.modelBV_RR3.mode <- P.modelBV_RR3.mode[1:2, 3]

posterior.mode(mcmc(S.modelBV_RR3))
HPDinterval(mcmc(S.modelBV_RR3))

# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
beta_post_RR3 <- matrix(NA, nrow(S.modelBV_RR3) ,2)

for (i in 1:nrow(S.modelBV_RR3)) {
  P3_3 <- matrix(rep(NA, 9), nrow = 3) 
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3_3[k] <- P.modelBV_RR3[i, k] }  
  P2_3 <- P3_3[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S3 <- P3_3[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_RR3[i,] <- solve(P2_3) %*% S3   # selection gradients beta = P^-1 * S
}

colnames(beta_post_RR3) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_RR3))
HPDinterval(mcmc(beta_post_RR3))
```

##### Negative binomial model

```{r}
# Extract posterior samples
bivar2.all.brm.nb_post <- posterior_samples(bivar2.all.brm.nb)
bivar2.all.brm.nb_post <- as.mcmc(bivar2.all.brm.nb_post)
#head(bivar2.all.brm.nb_post)[,1:20]

# [,5] sd_id__FFD_Intercept
# [,6] sd_id__FFD_cmean_4
# [,7] sd_id__roundmeanfitnessfl_Intercept 
# [,9] cor_id__FFD_Intercept__FFD_cmean_4
# [,10] cor_id__FFD_Intercept__roundmeanfitnessfl_Intercept
# [,11] cor_id__FFD_cmean_4__roundmeanfitnessfl_Intercept

sampled.gmat4 <- sample.gmat2(bivar2.all.brm.nb_post, replicates = 1000) 

sgmat4 <- lapply(sampled.gmat4, `[`, c('gmat')) #Get list 'gmat' from each list
sgmat4 <- unname(sapply(sgmat4, '[[', 1)) #Change to matrix

sgmat4 <- t(sgmat4)

P.modelBV_RR4 <- sgmat4
P.modelBV_RR4.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.modelBV_RR4.mode[k] <- posterior.mode(mcmc(sgmat4[,k]))

# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.modelBV_RR4 <- sgmat4[,c(3,6)]
colnames(S.modelBV_RR4) <- c("S_intercepts", "S_slopes")
S.modelBV_RR4.mode <- P.modelBV_RR4.mode[1:2, 3]

posterior.mode(mcmc(S.modelBV_RR4))
HPDinterval(mcmc(S.modelBV_RR4))

# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
beta_post_RR4 <- matrix(NA, nrow(S.modelBV_RR4) ,2)

for (i in 1:nrow(S.modelBV_RR4)) {
  P3_4 <- matrix(rep(NA, 9), nrow = 3) 
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3_4[k] <- P.modelBV_RR4[i, k] }  
  P2_4 <- P3_4[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S4 <- P3_4[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_RR4[i,] <- solve(P2_4) %*% S4   # selection gradients beta = P^-1 * S
}

colnames(beta_post_RR4) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_RR4))
HPDinterval(mcmc(beta_post_RR4))
```

### 3. mean_fitness_study, no condition variable

```{r message=FALSE, warning=FALSE}
bf_fitness_study <- bf(round(mean_fitness_study) ~  (1|ID1|id)) 
# Set up model formula
```

#### Poisson distribution

```{r, eval=FALSE, include=TRUE}
bivar3.all.brm.pois<-brm(bf_FFD+bf_fitness_study, 
                         family = c(gaussian,poisson), 
                         data = datadef,warmup = 1000,iter = 4000,chains=4,thin=2,
                         inits = "random",seed = 12345,cores = my.cores,
                         control = list(adapt_delta = 0.99))
```

```{r}
summary(bivar3.all.brm.pois)
```

#### Negative binomial distribution

```{r, eval=FALSE, include=TRUE}
bivar3.all.brm.nb<-brm(bf_FFD+bf_fitness_study, 
                       family = c(gaussian,negbinomial), 
                       data = datadef,warmup = 1000,iter = 4000,chains=4,thin=2,
                       inits = "random",seed = 12345,cores = my.cores,
                       control = list(adapt_delta = 0.99, max_treedepth = 15))
```

```{r}
summary(bivar3.all.brm.nb)
```

#### Model evaluation: Compare models

```{r fig.height=8, fig.width=6}
plot(bivar3.all.brm.pois) 
plot(bivar3.all.brm.nb)
```

Rhat

```{r}
mcmc_rhat_hist(rhat(bivar3.all.brm.pois))+theme_bw()
mcmc_rhat_hist(rhat(bivar3.all.brm.nb))+theme_bw()
```

Effective sample size: 
  
```{r}
mcmc_neff_hist(neff_ratio(bivar3.all.brm.pois))+theme_bw()
mcmc_neff_hist(neff_ratio(bivar3.all.brm.nb))+theme_bw()
```

Posterior predictive checks:  
  
```{r}
y3_fitness<-round(subset(datadef,!is.na(FFD))$mean_fitness_study)
yrep4_fitness_pois<-posterior_predict(bivar3.all.brm.pois, 
                                      draws = 500,resp="roundmeanfitnessstudy")
yrep4_FFD_pois<-posterior_predict(bivar3.all.brm.pois, 
                                  draws = 500,resp="FFD")
yrep4_fitness_nb<-posterior_predict(bivar3.all.brm.nb, 
                                    draws = 500,resp="roundmeanfitnessstudy")
yrep4_FFD_nb<-posterior_predict(bivar3.all.brm.nb, 
                                draws = 500,resp="FFD")
# matrices of draws from the posterior predictive distribution
```

Each row of the matrix is a draw from the posterior predictive distribution, i.e. a vector with one element for each of the data points in y.

Comparison of the distribution of y and the distributions of the simulated datasets in the yrep matrix

```{r}
ppc_dens_overlay(y3_fitness, yrep4_fitness_pois[1:500,])
ppc_dens_overlay(y3_fitness, yrep4_fitness_pois[1:500,])+xlim(0, 10)
ppc_dens_overlay(y3_fitness, yrep4_fitness_nb[1:500,])
ppc_dens_overlay(y3_fitness, yrep4_fitness_nb[1:500,])+xlim(0,10)
ppc_dens_overlay(y2_FFD, yrep4_FFD_pois[1:500,])
ppc_dens_overlay(y2_FFD, yrep4_FFD_nb[1:500,])
```

Poisson and negative binomial look pretty similar.

Separate histograms of y and some of the yrep datasets

```{r}
ppc_hist(y3_fitness, yrep4_fitness_pois[1:8, ])
ppc_hist(y3_fitness, yrep4_fitness_pois[1:8, ])+
  coord_cartesian(xlim = c(-1, 20))
ppc_hist(y3_fitness, yrep4_fitness_nb[1:8, ])
ppc_hist(y3_fitness, yrep4_fitness_nb[1:8, ])+
  coord_cartesian(xlim = c(-1, 20))
```

Here, negative binomial looks a bit better for fitness.

```{r message=FALSE, warning=FALSE}
ppc_stat(y3_fitness, yrep4_fitness_pois,stat="median")
ppc_stat(y3_fitness, yrep4_fitness_nb,stat="median")
ppc_stat(y3_fitness, yrep4_fitness_pois,stat="mean")
ppc_stat(y3_fitness, yrep4_fitness_nb,stat="mean")
ppc_stat(y3_fitness, yrep4_fitness_pois,stat="sd")
ppc_stat(y3_fitness, yrep4_fitness_nb,stat="sd")
ppc_stat(y3_fitness, yrep4_fitness_pois,stat="max")
ppc_stat(y3_fitness, yrep4_fitness_nb,stat="max")
```

Look at the distribution of the proportion of zeros over the replicated datasets from the posterior predictive distribution in yrep and compare to the proportion of observed zeros in y.

```{r}
ppc_stat(y3_fitness, yrep4_fitness_pois, stat = "prop_zero", binwidth = 0.005)
ppc_stat(y3_fitness, yrep4_fitness_nb, stat = "prop_zero", binwidth = 0.005)
```

They look quite similar.

Measure of fit: Bayesian R2

```{r}
bayes_R2(bivar3.all.brm.pois)
bayes_R2(bivar3.all.brm.nb)
```

Very similar.

Leave-one-out cross validation (LOO): 
  
```{r eval=FALSE, include=FALSE}
loo4_pois <- loo(bivar3.all.brm.pois, save_psis = TRUE, moment_match = TRUE)
loo4_nb <- loo(bivar3.all.brm.nb, save_psis = TRUE, moment_match = TRUE)
```

```{r}
plot(loo4_pois)
plot(loo4_nb)
```

LOO predictive checks

```{r}
ppc_loo_pit_overlay(y3_fitness, yrep4_fitness_pois, 
                    lw = weights(loo4_pois$psis_object))
ppc_loo_pit_qq(y3_fitness, yrep4_fitness_pois, 
               lw = weights(loo4_pois$psis_object))
ppc_loo_pit_qq(y3_fitness, yrep4_fitness_pois, 
               lw = weights(loo4_pois$psis_object),compare="normal")
ppc_loo_pit_overlay(y3_fitness, yrep4_fitness_nb, 
                    lw = weights(loo4_nb$psis_object))
ppc_loo_pit_qq(y3_fitness, yrep4_fitness_nb, 
               lw = weights(loo4_nb$psis_object))
ppc_loo_pit_qq(y3_fitness, yrep4_fitness_nb, 
               lw = weights(loo4_nb$psis_object),compare="normal")
```

#### Extract selection coefficients

##### Poisson model

```{r}
# Extract posterior samples
bivar3.all.brm.pois_post <- posterior_samples(bivar3.all.brm.pois)
bivar3.all.brm.pois_post <- as.mcmc(bivar3.all.brm.pois_post)
#head(bivar3.all.brm.pois_post)[,1:20]

# [,4] sd_id__FFD_Intercept
# [,5] sd_id__FFD_cmean_4
# [,6] sd_id__roundmeanfitnessfl_Intercept 
# [,8] cor_id__FFD_Intercept__FFD_cmean_4
# [,9] cor_id__FFD_Intercept__roundmeanfitnessfl_Intercept
# [,10] cor_id__FFD_cmean_4__roundmeanfitnessfl_Intercept

sampled.gmat5 <- sample.gmat1(bivar3.all.brm.pois_post, replicates = 1000) 

sgmat5 <- lapply(sampled.gmat5, `[`, c('gmat')) #Get list 'gmat' from each list
sgmat5 <- unname(sapply(sgmat5, '[[', 1)) #Change to matrix

sgmat5 <- t(sgmat5)

P.modelBV_RR5 <- sgmat5
P.modelBV_RR5.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.modelBV_RR5.mode[k] <- posterior.mode(mcmc(sgmat5[,k]))

# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.modelBV_RR5 <- sgmat5[,c(3,6)]
colnames(S.modelBV_RR5) <- c("S_intercepts", "S_slopes")
S.modelBV_RR5.mode <- P.modelBV_RR5.mode[1:2, 3]

posterior.mode(mcmc(S.modelBV_RR5))
HPDinterval(mcmc(S.modelBV_RR5))

# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
beta_post_RR5 <- matrix(NA, nrow(S.modelBV_RR5) ,2)

for (i in 1:nrow(S.modelBV_RR5)) {
  P3_5 <- matrix(rep(NA, 9), nrow = 3) 
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3_5[k] <- P.modelBV_RR5[i, k] }  
  P2_5 <- P3_5[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S5 <- P3_5[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_RR5[i,] <- solve(P2_5) %*% S5   # selection gradients beta = P^-1 * S
}

colnames(beta_post_RR5) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_RR5))
HPDinterval(mcmc(beta_post_RR5))
```

##### Negative binomial model

```{r}
# Extract posterior samples
bivar3.all.brm.nb_post <- posterior_samples(bivar3.all.brm.nb)
bivar3.all.brm.nb_post <- as.mcmc(bivar3.all.brm.nb_post)
#head(bivar3.all.brm.nb_post)[,1:20]

# [,4] sd_id__FFD_Intercept
# [,5] sd_id__FFD_cmean_4
# [,6] sd_id__roundmeanfitnessfl_Intercept 
# [,8] cor_id__FFD_Intercept__FFD_cmean_4
# [,9] cor_id__FFD_Intercept__roundmeanfitnessfl_Intercept
# [,10] cor_id__FFD_cmean_4__roundmeanfitnessfl_Intercept

sampled.gmat6 <- sample.gmat1(bivar3.all.brm.nb_post, replicates = 1000) 

sgmat6 <- lapply(sampled.gmat6, `[`, c('gmat')) #Get list 'gmat' from each list
sgmat6 <- unname(sapply(sgmat6, '[[', 1)) #Change to matrix

sgmat6 <- t(sgmat6)

P.modelBV_RR6 <- sgmat6
P.modelBV_RR6.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.modelBV_RR6.mode[k] <- posterior.mode(mcmc(sgmat6[,k]))

# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.modelBV_RR6 <- sgmat6[,c(3,6)]
colnames(S.modelBV_RR6) <- c("S_intercepts", "S_slopes")
S.modelBV_RR6.mode <- P.modelBV_RR6.mode[1:2, 3]

posterior.mode(mcmc(S.modelBV_RR6))
HPDinterval(mcmc(S.modelBV_RR6))

# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
beta_post_RR6 <- matrix(NA, nrow(S.modelBV_RR6) ,2)

for (i in 1:nrow(S.modelBV_RR6)) {
  P3_6 <- matrix(rep(NA, 9), nrow = 3) 
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3_6[k] <- P.modelBV_RR6[i, k] }  
  P2_6 <- P3_6[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S6 <- P3_6[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_RR6[i,] <- solve(P2_6) %*% S6   # selection gradients beta = P^-1 * S
}

colnames(beta_post_RR6) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_RR6))
HPDinterval(mcmc(beta_post_RR6))
```

### 4. mean_fitness_study, with shoot volume

```{r message=FALSE, warning=FALSE}
bf_fitness_study_shoot <- bf(round(mean_fitness_study) ~  
                               cn_shoot_vol_mean_sqrt +
                         (1|ID1|id)) # Set up model formula
```

#### Poisson distribution

```{r, eval=FALSE, include=TRUE}
bivar4.all.brm.pois<-brm(bf_FFD+bf_fitness_study_shoot, 
                       family = c(gaussian,poisson), 
                       data = datadef,warmup = 2000,iter = 6000,chains=4,thin=2,
                       inits = "random",seed = 12345,cores = my.cores,
                       control = list(adapt_delta = 0.99, max_treedepth = 15))
```

```{r}
summary(bivar4.all.brm.pois) 
```

#### Negative binomial distribution

```{r, eval=FALSE, include=TRUE}
bivar4.all.brm.nb<-brm(bf_FFD+bf_fitness_study_shoot, 
                       family = c(gaussian,negbinomial), 
                       data = datadef,warmup = 1000,iter = 4000,chains=4,thin=2,
                       inits = "random",seed = 12345,cores = my.cores)
```

```{r}
summary(bivar4.all.brm.nb) 
```

#### Model evaluation: Compare models

```{r fig.height=8, fig.width=6}
plot(bivar4.all.brm.pois) 
plot(bivar4.all.brm.nb)
```

Rhat

```{r}
mcmc_rhat_hist(rhat(bivar4.all.brm.pois))+theme_bw()
mcmc_rhat_hist(rhat(bivar4.all.brm.nb))+theme_bw()
```

Effective sample size: 
  
```{r}
mcmc_neff_hist(neff_ratio(bivar4.all.brm.pois))+theme_bw()
mcmc_neff_hist(neff_ratio(bivar4.all.brm.nb))+theme_bw()
```

Posterior predictive checks:  
  
```{r}
y3_FFD<-subset(datadef,!is.na(FFD)&
                           !is.na(cn_shoot_vol_mean_sqrt))$FFD 
y4_fitness<-round(subset(datadef,!is.na(FFD)&
                           !is.na(cn_shoot_vol_mean_sqrt))$mean_fitness_study)
yrep5_fitness_pois<-posterior_predict(bivar4.all.brm.pois, 
                                      draws = 500,resp="roundmeanfitnessstudy")
yrep5_FFD_pois<-posterior_predict(bivar4.all.brm.pois, 
                                  draws = 500,resp="FFD")
yrep5_fitness_nb<-posterior_predict(bivar4.all.brm.nb, 
                                    draws = 500,resp="roundmeanfitnessstudy")
yrep5_FFD_nb<-posterior_predict(bivar4.all.brm.nb, 
                                draws = 500,resp="FFD")
# matrices of draws from the posterior predictive distribution
```

Each row of the matrix is a draw from the posterior predictive distribution, i.e. a vector with one element for each of the data points in y.

Comparison of the distribution of y and the distributions of the simulated datasets in the yrep matrix

```{r}
ppc_dens_overlay(y4_fitness, yrep5_fitness_pois[1:500,])
ppc_dens_overlay(y4_fitness, yrep5_fitness_pois[1:500,])+xlim(0, 10)
ppc_dens_overlay(y4_fitness, yrep5_fitness_nb[1:500,])
ppc_dens_overlay(y4_fitness, yrep5_fitness_nb[1:500,])+xlim(0,10)
ppc_dens_overlay(y3_FFD, yrep5_FFD_pois[1:500,])
ppc_dens_overlay(y3_FFD, yrep5_FFD_nb[1:500,])
```

Poisson and negative binomial look pretty similar.

Separate histograms of y and some of the yrep datasets

```{r}
ppc_hist(y4_fitness, yrep5_fitness_pois[1:8, ])
ppc_hist(y4_fitness, yrep5_fitness_pois[1:8, ])+
  coord_cartesian(xlim = c(-1, 20))
ppc_hist(y4_fitness, yrep5_fitness_nb[1:8, ])
ppc_hist(y4_fitness, yrep5_fitness_nb[1:8, ])+
  coord_cartesian(xlim = c(-1, 20))
```

Here, negative binomial looks a bit better for fitness.

```{r message=FALSE, warning=FALSE}
ppc_stat(y4_fitness, yrep5_fitness_pois,stat="median")
ppc_stat(y4_fitness, yrep5_fitness_nb,stat="median")
ppc_stat(y4_fitness, yrep5_fitness_pois,stat="mean")
ppc_stat(y4_fitness, yrep5_fitness_nb,stat="mean")
ppc_stat(y4_fitness, yrep5_fitness_pois,stat="sd")
ppc_stat(y4_fitness, yrep5_fitness_nb,stat="sd")
ppc_stat(y4_fitness, yrep5_fitness_pois,stat="max")
ppc_stat(y4_fitness, yrep5_fitness_nb,stat="max")
```

Look at the distribution of the proportion of zeros over the replicated datasets from the posterior predictive distribution in yrep and compare to the proportion of observed zeros in y.

```{r}
ppc_stat(y4_fitness, yrep5_fitness_pois, stat = "prop_zero", binwidth = 0.005)
ppc_stat(y4_fitness, yrep5_fitness_nb, stat = "prop_zero", binwidth = 0.005)
```

They look quite similar.

Measure of fit: Bayesian R2

```{r}
bayes_R2(bivar4.all.brm.pois)
bayes_R2(bivar4.all.brm.nb)
```

Very similar.

Leave-one-out cross validation (LOO): 
  
```{r eval=FALSE, include=FALSE}
loo5_pois <- loo(bivar4.all.brm.pois, save_psis = TRUE, moment_match = TRUE)
loo5_nb <- loo(bivar4.all.brm.nb, save_psis = TRUE, moment_match = TRUE)
```

```{r}
plot(loo5_pois)
plot(loo5_nb)
```

LOO predictive checks

```{r}
ppc_loo_pit_overlay(y3_fitness, yrep5_fitness_pois, 
                    lw = weights(loo5_pois$psis_object))
ppc_loo_pit_qq(y3_fitness, yrep5_fitness_pois, 
               lw = weights(loo5_pois$psis_object))
ppc_loo_pit_qq(y3_fitness, yrep5_fitness_pois, 
               lw = weights(loo5_pois$psis_object),compare="normal")
ppc_loo_pit_overlay(y3_fitness, yrep5_fitness_nb, 
                    lw = weights(loo5_nb$psis_object))
ppc_loo_pit_qq(y3_fitness, yrep5_fitness_nb, 
               lw = weights(loo5_nb$psis_object))
ppc_loo_pit_qq(y3_fitness, yrep5_fitness_nb, 
               lw = weights(loo5_nb$psis_object),compare="normal")
```

#### Extract selection coefficients

##### Poisson model

```{r}
# Extract posterior samples
bivar4.all.brm.pois_post <- posterior_samples(bivar4.all.brm.pois)
bivar4.all.brm.pois_post <- as.mcmc(bivar4.all.brm.pois_post)
#head(bivar4.all.brm.pois_post)[,1:20]

# [,4] sd_id__FFD_Intercept
# [,5] sd_id__FFD_cmean_4
# [,6] sd_id__roundmeanfitnessfl_Intercept 
# [,8] cor_id__FFD_Intercept__FFD_cmean_4
# [,9] cor_id__FFD_Intercept__roundmeanfitnessfl_Intercept
# [,10] cor_id__FFD_cmean_4__roundmeanfitnessfl_Intercept

sampled.gmat7 <- sample.gmat2(bivar4.all.brm.pois_post, replicates = 1000) 

sgmat7 <- lapply(sampled.gmat7, `[`, c('gmat')) #Get list 'gmat' from each list
sgmat7 <- unname(sapply(sgmat7, '[[', 1)) #Change to matrix

sgmat7 <- t(sgmat7)

P.modelBV_RR7 <- sgmat7
P.modelBV_RR7.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.modelBV_RR7.mode[k] <- posterior.mode(mcmc(sgmat7[,k]))

# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.modelBV_RR7 <- sgmat7[,c(3,6)]
colnames(S.modelBV_RR7) <- c("S_intercepts", "S_slopes")
S.modelBV_RR7.mode <- P.modelBV_RR7.mode[1:2, 3]

posterior.mode(mcmc(S.modelBV_RR7))
HPDinterval(mcmc(S.modelBV_RR7))

# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
beta_post_RR7 <- matrix(NA, nrow(S.modelBV_RR7) ,2)

for (i in 1:nrow(S.modelBV_RR7)) {
  P3_7 <- matrix(rep(NA, 9), nrow = 3) 
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3_7[k] <- P.modelBV_RR7[i, k] }  
  P2_7 <- P3_7[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S7 <- P3_7[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_RR7[i,] <- solve(P2_7) %*% S7   # selection gradients beta = P^-1 * S
}

colnames(beta_post_RR7) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_RR7))
HPDinterval(mcmc(beta_post_RR7))
```

##### Negative binomial model

```{r}
# Extract posterior samples
bivar4.all.brm.nb_post <- posterior_samples(bivar4.all.brm.nb)
bivar4.all.brm.nb_post <- as.mcmc(bivar4.all.brm.nb_post)
#head(bivar4.all.brm.nb_post)[,1:20]

# [,4] sd_id__FFD_Intercept
# [,5] sd_id__FFD_cmean_4
# [,6] sd_id__roundmeanfitnessfl_Intercept 
# [,8] cor_id__FFD_Intercept__FFD_cmean_4
# [,9] cor_id__FFD_Intercept__roundmeanfitnessfl_Intercept
# [,10] cor_id__FFD_cmean_4__roundmeanfitnessfl_Intercept

sampled.gmat8 <- sample.gmat2(bivar4.all.brm.nb_post, replicates = 1000) 

sgmat8 <- lapply(sampled.gmat8, `[`, c('gmat')) #Get list 'gmat' from each list
sgmat8 <- unname(sapply(sgmat8, '[[', 1)) #Change to matrix

sgmat8 <- t(sgmat8)

P.modelBV_RR8 <- sgmat8
P.modelBV_RR8.mode <- matrix(1:9, nrow = 3)
for (k in 1:9) P.modelBV_RR8.mode[k] <- posterior.mode(mcmc(sgmat8[,k]))

# Extract selection *differentials* (i.e. covariances) for intercept and slope:
# and calculate posterior mode and credible intervals for each
S.modelBV_RR8 <- sgmat8[,c(3,6)]
colnames(S.modelBV_RR8) <- c("S_intercepts", "S_slopes")
S.modelBV_RR8.mode <- P.modelBV_RR8.mode[1:2, 3]

posterior.mode(mcmc(S.modelBV_RR8))
HPDinterval(mcmc(S.modelBV_RR8))

# Estimate selection gradients for intercept and slope (beta = S / P)
# on each sample of posterior and extract their mode
beta_post_RR8 <- matrix(NA, nrow(S.modelBV_RR8) ,2)

for (i in 1:nrow(S.modelBV_RR8)) {
  P3_8 <- matrix(rep(NA, 9), nrow = 3) 
  # 3x3 matrix of var-cov for individual X.int, X.slope and fitness
  for (k in 1:9) {P3_8[k] <- P.modelBV_RR8[i, k] }  
  P2_8 <- P3_8[1:2, 1:2]   # 2x2 matrix of just trait intercept & slope var-cov
  S8 <- P3_8[1:2, 3]   # selection differentials on traits (last column of P3)
  beta_post_RR8[i,] <- solve(P2_8) %*% S8   # selection gradients beta = P^-1 * S
}

colnames(beta_post_RR8) <- c("beta_intercepts", "beta_slopes")
posterior.mode(mcmc(beta_post_RR8))
HPDinterval(mcmc(beta_post_RR8))
```

# Compare results of MCMCglmm and brms

## Univariate models

Fixed effects can be compared directly between MCMCglmm and brms outputs.

From Pieter's email: The group-level (random) effects reported by brms are standard deviations and correlations rather than the variances and covariances that mcmcglmm outputs. We need to square all the mcmc samples from the posterior then take their mean to get true estimates of the variance from brms (then we can compare them).

Code was checked by Piet.

### FFD with random effects of year and individual-intercept

```{r}
kable(rbind(MCMCglmm=summary(univar.FFD.all)$solutions[,1],
      brms=summary(univar.FFD.all.brm)$fixed[,1])) # Comparison fixed effects

univar.FFD.all.brm_asmcmc <- as.mcmc(univar.FFD.all.brm, combine_chains = TRUE)
#head(univar.FFD.all.brm_asmcmc) # check which column the parameters are in

univar.FFD.all.brm_year <- (univar.FFD.all.brm_asmcmc[,4]^2)  
# sd_year__Intercept^2
univar.FFD.all.brm_id_intercept <- (univar.FFD.all.brm_asmcmc[,3]^2)  
# sd_id__Intercept^2 (individual intercept)
univar.FFD.all.brm_resid <- (univar.FFD.all.brm_asmcmc[,5]^2) 
# sigma^2 (residual)

kable(rbind(MCMCglmm=summary(univar.FFD.all$VCV)$statistics[,1],
      brms=as.vector(cbind(mean(univar.FFD.all.brm_year),
                           mean(univar.FFD.all.brm_id_intercept),
            mean(univar.FFD.all.brm_resid))))) # Comparison random effects

# Calculate 95% CI interval of the converted values
# round(HPDinterval(univar.FFD.all.brm_id_intercept), 2)
# round(HPDinterval(univar.FFD.all.brm_year), 2)
# round(HPDinterval(univar.FFD.all.brm_resid), 2)
```

Quite similar values.

### Random regression for FFD, including random effects of individual slopes and covariance between intercept and slope

```{r}
kable(rbind(MCMCglmm=summary(univar.FFD_RR.all)$solutions[,1],
      brms=summary(univar.FFD_RR.all.brm)$fixed[,1])) # Comparison fixed effects

univar.FFD_RR.all.brm_asmcmc <- as.mcmc(univar.FFD_RR.all.brm, 
                                        combine_chains = TRUE)
#head(univar.FFD_RR.all.brm_asmcmc) # check which column the parameters are in

univar.FFD_RR.all.brm_year <- (univar.FFD_RR.all.brm_asmcmc[,5]^2)  
# sd_year__Intercept^2
univar.FFD_RR.all.brm_id_intercept <- (univar.FFD_RR.all.brm_asmcmc[,3]^2)  
# sd_id__Intercept^2 (individual intercept)
univar.FFD_RR.all.brm_year_cor_id_intercept_cmean_4<-(univar.FFD_RR.all.brm_asmcmc[,6]^2) 
#cor_id__Intercept__cmean_4^2 (corr intercept-slope)
univar.FFD_RR.all.brm_year_sd_id_cmean_4<-(univar.FFD_RR.all.brm_asmcmc[,4]^2)
#sd_id__cmean_4^2 (individual slope)
univar.FFD_RR.all.brm_resid <- (univar.FFD_RR.all.brm_asmcmc[,7]^2) 
# sigma^2 (residual)

kable(cbind(MCMCglmm=summary(univar.FFD_RR.all$VCV)$statistics[,1],
      brms=as.vector(cbind(mean(univar.FFD_RR.all.brm_year),
                           mean(univar.FFD_RR.all.brm_id_intercept),
                           mean(univar.FFD_RR.all.brm_year_cor_id_intercept_cmean_4),
                           mean(univar.FFD_RR.all.brm_year_cor_id_intercept_cmean_4),
                           mean(univar.FFD_RR.all.brm_year_sd_id_cmean_4),
                           mean(univar.FFD_RR.all.brm_resid))))) 
# Comparison random effects
```


## Bivariate models

I am not sure the code for this comparison is correct, the code needs checking! Therefore, I only performed the comparison for models with mean fitness per flowering event and no condition variable so far.

### 1. mean_fitness_fl, no condition variable

```{r}
kable(data.frame(summary(bivar1.all)$solutions)[1]) # Fixed effects MCMCglmm
kable(data.frame(summary(bivar1.all.brm.nb)$fixed)[1]) # Fixed effects brms

bivar1.all.brm.nb_asmcmc <- as.mcmc(bivar1.all.brm.nb, combine_chains = TRUE)
#head(bivar1.all.brm.nb_asmcmc) # check which column the parameters are in

bivar1.all.brm.nb_year <- (bivar1.all.brm.nb_asmcmc[,7]^2)  
# sd_year__Intercept^2

bivar1.all.brm.nb_id_intercept_FFD <- (bivar1.all.brm.nb_asmcmc[,4]^2)  
# sd(FFD_Intercept)^2 (individual intercept for FFD)

bivar1.all.brm.nb_cor_intercept_slope<-(bivar1.all.brm.nb_asmcmc[,8]^2) 
# cor(FFD_Intercept,FFD_cmean_4)^2 (corr intercept for FFD - slope for FFD)

bivar1.all.brm.nb_cor_intercept_fitness<-(bivar1.all.brm.nb_asmcmc[,9]^2) 
# cor(roundmeanfitnessfl_Intercept,FFD_Intercept)^2 (corr intercept FFD - fitness)

# cor(FFD_Intercept,FFD_cmean_4)^2 (corr intercept for FFD - slope for FFD) (rep)

bivar1.all.brm.nb_id_slope_FFD <- (bivar1.all.brm.nb_asmcmc[,5]^2)  
# sd(FFD_cmean_4)^2 (individual slope for FFD)

bivar1.all.brm.nb_cor_slope_fitness<-(bivar1.all.brm.nb_asmcmc[,10]^2) 
# cor(roundmeanfitnessfl_Intercept,FFD_cmean_4)^2 (corr slope for FFD - fitness)

# cor(roundmeanfitnessfl_Intercept,FFD_Intercept)^2 (corr intercept for FFD - fitness) (rep)

# cor(roundmeanfitnessfl_Intercept,FFD_cmean_4)^2 (corr slope for FFD - fitness) (rep)

bivar1.all.brm.nb_intercept_fitness<-(bivar1.all.brm.nb_asmcmc[,6]^2) 
# sd(roundmeanfitnessfl_Intercept)^2 (intercept for fitness)

bivar1.all.brm.nb_resid<-(bivar1.all.brm.nb_asmcmc[,11]^2) 
# sigma_FFD^2 (residual)

compar_bivar1<-cbind(MCMCglmm=summary(bivar1.all$VCV)$statistics[,1],
      brms=as.vector(cbind(mean(bivar1.all.brm.nb_year),
                           mean(bivar1.all.brm.nb_id_intercept_FFD),
                           mean(bivar1.all.brm.nb_cor_intercept_slope),
                           -mean(bivar1.all.brm.nb_cor_intercept_fitness),
                           mean(bivar1.all.brm.nb_cor_intercept_slope),
                           mean(bivar1.all.brm.nb_id_slope_FFD),
                           -mean(bivar1.all.brm.nb_cor_slope_fitness),
                           -mean(bivar1.all.brm.nb_cor_intercept_fitness),
                           -mean(bivar1.all.brm.nb_cor_slope_fitness),
                           mean(bivar1.all.brm.nb_intercept_fitness),
                           mean(bivar1.all.brm.nb_resid))))
# Comparison random effects
compar_bivar1<-compar_bivar1[c(1:4,6:7,10:11),]
row.names(compar_bivar1)<-c("year_FFD",
                          "id_var_intercept_FFD",
                          "id_covar_intercept_slope_FFD",
                          "id_covar_fitness_intercept_FFD",
                          "id_var_slope_FFD",
                          "id_covar_fitness_slope_FFD",
                          "id_var_intercept_fitness",
                          "residual")
kable(compar_bivar1)
```

# Variation in selection among years with BLUPs

Add BLUPs to data set

```{r}
datadef<-datadef%>%left_join(BLUPs_MCMC.all.brms%>%
                               select(intercept,slope)%>%
                               rownames_to_column(var="id"))
```

## Temp*slope

```{r}
modelBLUP_1<-brm(formula=round(intactseed)~slope*cmean_4+(1|id),
                  family="poisson",data=datadef,
                  warmup = 1000,iter = 4000,thin=2,chains=4,
                  inits = "random",seed = 12345,cores = my.cores)
```

```{r}
conditional_effects(modelBLUP_1)
```

## Temp*slope+volume

```{r}
modelBLUP_2<-brm(formula=round(intactseed)~slope*cmean_4+
                   cn_shoot_vol_mean_sqrt +(1|id),
                  family="poisson",data=datadef,
                  warmup = 1000,iter = 4000,thin=2,chains=4,
                  inits = "random",seed = 12345,cores = my.cores)
```

```{r}
conditional_effects(modelBLUP_2)
```

## Volume*slope

```{r}
modelBLUP_3<-brm(formula=round(intactseed)~slope*cn_shoot_vol_mean_sqrt+(1|id),
                  family="poisson",data=datadef,
                  warmup = 1000,iter = 4000,thin=2,chains=4,
                  inits = "random",seed = 12345,cores = my.cores)
```

```{r}
conditional_effects(modelBLUP_3)
```

## Temp*slope+volume*slope

```{r}
modelBLUP_4<-brm(formula=round(intactseed)~slope*cmean_4+
                   slope*cn_shoot_vol_mean_sqrt+(1|id),
                  family="poisson",data=datadef,
                  warmup = 1000,iter = 4000,thin=2,chains=4,
                  inits = "random",seed = 12345,cores = my.cores)
```

```{r}
conditional_effects(modelBLUP_4)
```

# Save large objects as .RData file

```{r eval=FALSE, include=FALSE}
save(univar.FFD_yearonly.all.brm,
     univar.FFD.all, univar.FFD_RR.all, 
     univar.FFD.all.brm,univar.FFD_RR.all.brm,loo1,
     bivar1.all,bivar2.all,bivar3.all,bivar4.all,
     bivar1.all.brm.pois,bivar1.all.brm.nb,
     loo2_pois,loo2_nb,kfold1_pois,kfold1_nb,
     bivar1.all.brm.pois_priors, 
     bivar2.all.brm.pois,bivar2.all.brm.nb,
     bivar3.all.brm.pois,bivar3.all.brm.nb,
     bivar4.all.brm.pois,bivar4.all.brm.nb,
     modelBLUP_1,modelBLUP_2,modelBLUP_3,modelBLUP_4,
     file = "output/large_objects.RData")
```

TO DO: run again the analyses with the BLUPs to test for variation in selection among years with all the individuals

```{r include=FALSE}
sessionInfo()
```
